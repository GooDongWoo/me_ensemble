{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 모델 로드 및 전처리 등 실험과 상관 없는 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models,datasets, transforms\n",
    "from mevit_model import MultiExitViT\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from temperature_scaling import TemperatureScaling\n",
    "from Dloaders import Dloaders\n",
    "####################################################################\n",
    "IMG_SIZE = 224\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dataset_name = {'cifar10':datasets.CIFAR10, 'cifar100':datasets.CIFAR100,'imagenet':None}\n",
    "dataset_outdim = {'cifar10':10, 'cifar100':100,'imagenet':1000}\n",
    "##############################################################\n",
    "################ 0. Hyperparameters ##########################\n",
    "##############################################################\n",
    "batch_size = 1024\n",
    "data_choice='cifar10'\n",
    "mevit_isload=True\n",
    "mevit_pretrained_path=f\"models/{data_choice}/integrated_ee.pth\"\n",
    "\n",
    "backbone_path=f'models/{data_choice}/vit_{data_choice}_backbone.pth'\n",
    "start_lr=1e-4\n",
    "max_iter=200\n",
    "\n",
    "ee_list=[0,1,2,3,4,5,6,7,8,9]#exit list ex) [0,1,2,3,4,5,6,7,8,9]\n",
    "exit_loss_weights=[1,1,1,1,1,1,1,1,1,1,1]#exit마다 가중치\n",
    "exit_num=11\n",
    "##############################################################\n",
    "dloaders=Dloaders(data_choice=data_choice,batch_size=batch_size,IMG_SIZE=IMG_SIZE)\n",
    "train_loader,test_loader = dloaders.get_loaders()\n",
    "\n",
    "# Load the pretrained ViT model from the saved file\n",
    "pretrained_vit = models.vit_b_16(weights=models.ViT_B_16_Weights.DEFAULT)\n",
    "if data_choice != 'imagenet':\n",
    "    pretrained_vit.heads.head = nn.Linear(pretrained_vit.heads.head.in_features, dataset_outdim[data_choice])  # Ensure output matches the number of classes\n",
    "\n",
    "    # Load model weights\n",
    "    pretrained_vit.load_state_dict(torch.load(backbone_path))\n",
    "    pretrained_vit = pretrained_vit.to(device)\n",
    "#from torchinfo import summary\n",
    "#summary(pretrained_vit,input_size= (64, 3, IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "model = MultiExitViT(pretrained_vit,num_classes=dataset_outdim[data_choice],ee_list=ee_list,exit_loss_weights=exit_loss_weights).to(device)\n",
    "\n",
    "# Assume a pretrained model (replace with your own model)\n",
    "model.load_state_dict(torch.load(mevit_pretrained_path))  # Load your trained weights\n",
    "file_path = f'cache_result_mevit_{data_choice}.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. MEVIT의 각 출구들에서의 Precision을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_accs = [72.76, 80.35, 85.8, 89.42, 91.98, 93.12, 94.94, 96.44, 96.95, 97.35, 97.63]\n",
    "cifar100_accs = [48.21, 59.63, 67.93, 73.31, 77.9, 80.8, 83.71, 85.33, 86.61, 87.04, 87.75]\n",
    "imagenet_accs = [34.76, 42.65, 51.82, 57.76, 62.19, 65.5, 69.29, 72.3, 75.3, 77.6, 81.06]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MEVIT의 각 출구들까지의 FLOPs 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_FLOPs_list = [2908083456, 4303940352, 5699797248, 7095654144, 8491511040, 9887367936, 11283224832, 12679081728, 14074938624, 15470795520, 16866652416]\n",
    "cifar100_FLOPs_list = [2908152576, 4304009472, 5699866368, 7095723264, 8491580160, 9887437056, 11283293952, 12679150848, 14075007744, 15470864640, 16866721536]\n",
    "#imagenet_FLOPs_list="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::mul encountered 89 time(s)\n",
      "Unsupported operator aten::add encountered 45 time(s)\n",
      "Unsupported operator aten::div encountered 22 time(s)\n",
      "Unsupported operator aten::unflatten encountered 22 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 22 time(s)\n",
      "Unsupported operator aten::gelu encountered 22 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "base_model.encoder.layers.encoder_layer_0.self_attention.out_proj, base_model.encoder.layers.encoder_layer_1.self_attention.out_proj, base_model.encoder.layers.encoder_layer_10.self_attention.out_proj, base_model.encoder.layers.encoder_layer_11.self_attention.out_proj, base_model.encoder.layers.encoder_layer_2.self_attention.out_proj, base_model.encoder.layers.encoder_layer_3.self_attention.out_proj, base_model.encoder.layers.encoder_layer_4.self_attention.out_proj, base_model.encoder.layers.encoder_layer_5.self_attention.out_proj, base_model.encoder.layers.encoder_layer_6.self_attention.out_proj, base_model.encoder.layers.encoder_layer_7.self_attention.out_proj, base_model.encoder.layers.encoder_layer_8.self_attention.out_proj, base_model.encoder.layers.encoder_layer_9.self_attention.out_proj, ees.0.0.self_attention.out_proj, ees.1.0.self_attention.out_proj, ees.2.0.self_attention.out_proj, ees.3.0.self_attention.out_proj, ees.4.0.self_attention.out_proj, ees.5.0.self_attention.out_proj, ees.6.0.self_attention.out_proj, ees.7.0.self_attention.out_proj, ees.8.0.self_attention.out_proj, ees.9.0.self_attention.out_proj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30832862976\n"
     ]
    }
   ],
   "source": [
    "from fvcore.nn import FlopCountAnalysis\n",
    "x = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(device)\n",
    "flops = FlopCountAnalysis(model, x)\n",
    "print(flops.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::mul encountered 9 time(s)\n",
      "Unsupported operator aten::add encountered 5 time(s)\n",
      "Unsupported operator aten::div encountered 2 time(s)\n",
      "Unsupported operator aten::unflatten encountered 2 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 2 time(s)\n",
      "Unsupported operator aten::gelu encountered 2 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "base_model.encoder.layers.encoder_layer_0.self_attention.out_proj, base_model.encoder.layers.encoder_layer_1, base_model.encoder.layers.encoder_layer_1.dropout, base_model.encoder.layers.encoder_layer_1.ln_1, base_model.encoder.layers.encoder_layer_1.ln_2, base_model.encoder.layers.encoder_layer_1.mlp, base_model.encoder.layers.encoder_layer_1.mlp.0, base_model.encoder.layers.encoder_layer_1.mlp.1, base_model.encoder.layers.encoder_layer_1.mlp.2, base_model.encoder.layers.encoder_layer_1.mlp.3, base_model.encoder.layers.encoder_layer_1.mlp.4, base_model.encoder.layers.encoder_layer_1.self_attention, base_model.encoder.layers.encoder_layer_1.self_attention.out_proj, base_model.encoder.layers.encoder_layer_10, base_model.encoder.layers.encoder_layer_10.dropout, base_model.encoder.layers.encoder_layer_10.ln_1, base_model.encoder.layers.encoder_layer_10.ln_2, base_model.encoder.layers.encoder_layer_10.mlp, base_model.encoder.layers.encoder_layer_10.mlp.0, base_model.encoder.layers.encoder_layer_10.mlp.1, base_model.encoder.layers.encoder_layer_10.mlp.2, base_model.encoder.layers.encoder_layer_10.mlp.3, base_model.encoder.layers.encoder_layer_10.mlp.4, base_model.encoder.layers.encoder_layer_10.self_attention, base_model.encoder.layers.encoder_layer_10.self_attention.out_proj, base_model.encoder.layers.encoder_layer_11, base_model.encoder.layers.encoder_layer_11.dropout, base_model.encoder.layers.encoder_layer_11.ln_1, base_model.encoder.layers.encoder_layer_11.ln_2, base_model.encoder.layers.encoder_layer_11.mlp, base_model.encoder.layers.encoder_layer_11.mlp.0, base_model.encoder.layers.encoder_layer_11.mlp.1, base_model.encoder.layers.encoder_layer_11.mlp.2, base_model.encoder.layers.encoder_layer_11.mlp.3, base_model.encoder.layers.encoder_layer_11.mlp.4, base_model.encoder.layers.encoder_layer_11.self_attention, base_model.encoder.layers.encoder_layer_11.self_attention.out_proj, base_model.encoder.layers.encoder_layer_2, base_model.encoder.layers.encoder_layer_2.dropout, base_model.encoder.layers.encoder_layer_2.ln_1, base_model.encoder.layers.encoder_layer_2.ln_2, base_model.encoder.layers.encoder_layer_2.mlp, base_model.encoder.layers.encoder_layer_2.mlp.0, base_model.encoder.layers.encoder_layer_2.mlp.1, base_model.encoder.layers.encoder_layer_2.mlp.2, base_model.encoder.layers.encoder_layer_2.mlp.3, base_model.encoder.layers.encoder_layer_2.mlp.4, base_model.encoder.layers.encoder_layer_2.self_attention, base_model.encoder.layers.encoder_layer_2.self_attention.out_proj, base_model.encoder.layers.encoder_layer_3, base_model.encoder.layers.encoder_layer_3.dropout, base_model.encoder.layers.encoder_layer_3.ln_1, base_model.encoder.layers.encoder_layer_3.ln_2, base_model.encoder.layers.encoder_layer_3.mlp, base_model.encoder.layers.encoder_layer_3.mlp.0, base_model.encoder.layers.encoder_layer_3.mlp.1, base_model.encoder.layers.encoder_layer_3.mlp.2, base_model.encoder.layers.encoder_layer_3.mlp.3, base_model.encoder.layers.encoder_layer_3.mlp.4, base_model.encoder.layers.encoder_layer_3.self_attention, base_model.encoder.layers.encoder_layer_3.self_attention.out_proj, base_model.encoder.layers.encoder_layer_4, base_model.encoder.layers.encoder_layer_4.dropout, base_model.encoder.layers.encoder_layer_4.ln_1, base_model.encoder.layers.encoder_layer_4.ln_2, base_model.encoder.layers.encoder_layer_4.mlp, base_model.encoder.layers.encoder_layer_4.mlp.0, base_model.encoder.layers.encoder_layer_4.mlp.1, base_model.encoder.layers.encoder_layer_4.mlp.2, base_model.encoder.layers.encoder_layer_4.mlp.3, base_model.encoder.layers.encoder_layer_4.mlp.4, base_model.encoder.layers.encoder_layer_4.self_attention, base_model.encoder.layers.encoder_layer_4.self_attention.out_proj, base_model.encoder.layers.encoder_layer_5, base_model.encoder.layers.encoder_layer_5.dropout, base_model.encoder.layers.encoder_layer_5.ln_1, base_model.encoder.layers.encoder_layer_5.ln_2, base_model.encoder.layers.encoder_layer_5.mlp, base_model.encoder.layers.encoder_layer_5.mlp.0, base_model.encoder.layers.encoder_layer_5.mlp.1, base_model.encoder.layers.encoder_layer_5.mlp.2, base_model.encoder.layers.encoder_layer_5.mlp.3, base_model.encoder.layers.encoder_layer_5.mlp.4, base_model.encoder.layers.encoder_layer_5.self_attention, base_model.encoder.layers.encoder_layer_5.self_attention.out_proj, base_model.encoder.layers.encoder_layer_6, base_model.encoder.layers.encoder_layer_6.dropout, base_model.encoder.layers.encoder_layer_6.ln_1, base_model.encoder.layers.encoder_layer_6.ln_2, base_model.encoder.layers.encoder_layer_6.mlp, base_model.encoder.layers.encoder_layer_6.mlp.0, base_model.encoder.layers.encoder_layer_6.mlp.1, base_model.encoder.layers.encoder_layer_6.mlp.2, base_model.encoder.layers.encoder_layer_6.mlp.3, base_model.encoder.layers.encoder_layer_6.mlp.4, base_model.encoder.layers.encoder_layer_6.self_attention, base_model.encoder.layers.encoder_layer_6.self_attention.out_proj, base_model.encoder.layers.encoder_layer_7, base_model.encoder.layers.encoder_layer_7.dropout, base_model.encoder.layers.encoder_layer_7.ln_1, base_model.encoder.layers.encoder_layer_7.ln_2, base_model.encoder.layers.encoder_layer_7.mlp, base_model.encoder.layers.encoder_layer_7.mlp.0, base_model.encoder.layers.encoder_layer_7.mlp.1, base_model.encoder.layers.encoder_layer_7.mlp.2, base_model.encoder.layers.encoder_layer_7.mlp.3, base_model.encoder.layers.encoder_layer_7.mlp.4, base_model.encoder.layers.encoder_layer_7.self_attention, base_model.encoder.layers.encoder_layer_7.self_attention.out_proj, base_model.encoder.layers.encoder_layer_8, base_model.encoder.layers.encoder_layer_8.dropout, base_model.encoder.layers.encoder_layer_8.ln_1, base_model.encoder.layers.encoder_layer_8.ln_2, base_model.encoder.layers.encoder_layer_8.mlp, base_model.encoder.layers.encoder_layer_8.mlp.0, base_model.encoder.layers.encoder_layer_8.mlp.1, base_model.encoder.layers.encoder_layer_8.mlp.2, base_model.encoder.layers.encoder_layer_8.mlp.3, base_model.encoder.layers.encoder_layer_8.mlp.4, base_model.encoder.layers.encoder_layer_8.self_attention, base_model.encoder.layers.encoder_layer_8.self_attention.out_proj, base_model.encoder.layers.encoder_layer_9, base_model.encoder.layers.encoder_layer_9.dropout, base_model.encoder.layers.encoder_layer_9.ln_1, base_model.encoder.layers.encoder_layer_9.ln_2, base_model.encoder.layers.encoder_layer_9.mlp, base_model.encoder.layers.encoder_layer_9.mlp.0, base_model.encoder.layers.encoder_layer_9.mlp.1, base_model.encoder.layers.encoder_layer_9.mlp.2, base_model.encoder.layers.encoder_layer_9.mlp.3, base_model.encoder.layers.encoder_layer_9.mlp.4, base_model.encoder.layers.encoder_layer_9.self_attention, base_model.encoder.layers.encoder_layer_9.self_attention.out_proj, base_model.encoder.ln, base_model.heads, base_model.heads.head, classifiers.1, classifiers.2, classifiers.3, classifiers.4, classifiers.5, classifiers.6, classifiers.7, classifiers.8, classifiers.9, ees.0.0.self_attention.out_proj, ees.1, ees.1.0, ees.1.0.dropout, ees.1.0.ln_1, ees.1.0.ln_2, ees.1.0.mlp, ees.1.0.mlp.0, ees.1.0.mlp.1, ees.1.0.mlp.2, ees.1.0.mlp.3, ees.1.0.mlp.4, ees.1.0.self_attention, ees.1.0.self_attention.out_proj, ees.1.1, ees.2, ees.2.0, ees.2.0.dropout, ees.2.0.ln_1, ees.2.0.ln_2, ees.2.0.mlp, ees.2.0.mlp.0, ees.2.0.mlp.1, ees.2.0.mlp.2, ees.2.0.mlp.3, ees.2.0.mlp.4, ees.2.0.self_attention, ees.2.0.self_attention.out_proj, ees.2.1, ees.3, ees.3.0, ees.3.0.dropout, ees.3.0.ln_1, ees.3.0.ln_2, ees.3.0.mlp, ees.3.0.mlp.0, ees.3.0.mlp.1, ees.3.0.mlp.2, ees.3.0.mlp.3, ees.3.0.mlp.4, ees.3.0.self_attention, ees.3.0.self_attention.out_proj, ees.3.1, ees.4, ees.4.0, ees.4.0.dropout, ees.4.0.ln_1, ees.4.0.ln_2, ees.4.0.mlp, ees.4.0.mlp.0, ees.4.0.mlp.1, ees.4.0.mlp.2, ees.4.0.mlp.3, ees.4.0.mlp.4, ees.4.0.self_attention, ees.4.0.self_attention.out_proj, ees.4.1, ees.5, ees.5.0, ees.5.0.dropout, ees.5.0.ln_1, ees.5.0.ln_2, ees.5.0.mlp, ees.5.0.mlp.0, ees.5.0.mlp.1, ees.5.0.mlp.2, ees.5.0.mlp.3, ees.5.0.mlp.4, ees.5.0.self_attention, ees.5.0.self_attention.out_proj, ees.5.1, ees.6, ees.6.0, ees.6.0.dropout, ees.6.0.ln_1, ees.6.0.ln_2, ees.6.0.mlp, ees.6.0.mlp.0, ees.6.0.mlp.1, ees.6.0.mlp.2, ees.6.0.mlp.3, ees.6.0.mlp.4, ees.6.0.self_attention, ees.6.0.self_attention.out_proj, ees.6.1, ees.7, ees.7.0, ees.7.0.dropout, ees.7.0.ln_1, ees.7.0.ln_2, ees.7.0.mlp, ees.7.0.mlp.0, ees.7.0.mlp.1, ees.7.0.mlp.2, ees.7.0.mlp.3, ees.7.0.mlp.4, ees.7.0.self_attention, ees.7.0.self_attention.out_proj, ees.7.1, ees.8, ees.8.0, ees.8.0.dropout, ees.8.0.ln_1, ees.8.0.ln_2, ees.8.0.mlp, ees.8.0.mlp.0, ees.8.0.mlp.1, ees.8.0.mlp.2, ees.8.0.mlp.3, ees.8.0.mlp.4, ees.8.0.self_attention, ees.8.0.self_attention.out_proj, ees.8.1, ees.9, ees.9.0, ees.9.0.dropout, ees.9.0.ln_1, ees.9.0.ln_2, ees.9.0.mlp, ees.9.0.mlp.0, ees.9.0.mlp.1, ees.9.0.mlp.2, ees.9.0.mlp.3, ees.9.0.mlp.4, ees.9.0.self_attention, ees.9.0.self_attention.out_proj, ees.9.1\n",
      "Unsupported operator aten::mul encountered 13 time(s)\n",
      "Unsupported operator aten::add encountered 7 time(s)\n",
      "Unsupported operator aten::div encountered 3 time(s)\n",
      "Unsupported operator aten::unflatten encountered 3 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 3 time(s)\n",
      "Unsupported operator aten::gelu encountered 3 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "base_model.encoder.layers.encoder_layer_0.self_attention.out_proj, base_model.encoder.layers.encoder_layer_1.self_attention.out_proj, base_model.encoder.layers.encoder_layer_10, base_model.encoder.layers.encoder_layer_10.dropout, base_model.encoder.layers.encoder_layer_10.ln_1, base_model.encoder.layers.encoder_layer_10.ln_2, base_model.encoder.layers.encoder_layer_10.mlp, base_model.encoder.layers.encoder_layer_10.mlp.0, base_model.encoder.layers.encoder_layer_10.mlp.1, base_model.encoder.layers.encoder_layer_10.mlp.2, base_model.encoder.layers.encoder_layer_10.mlp.3, base_model.encoder.layers.encoder_layer_10.mlp.4, base_model.encoder.layers.encoder_layer_10.self_attention, base_model.encoder.layers.encoder_layer_10.self_attention.out_proj, base_model.encoder.layers.encoder_layer_11, base_model.encoder.layers.encoder_layer_11.dropout, base_model.encoder.layers.encoder_layer_11.ln_1, base_model.encoder.layers.encoder_layer_11.ln_2, base_model.encoder.layers.encoder_layer_11.mlp, base_model.encoder.layers.encoder_layer_11.mlp.0, base_model.encoder.layers.encoder_layer_11.mlp.1, base_model.encoder.layers.encoder_layer_11.mlp.2, base_model.encoder.layers.encoder_layer_11.mlp.3, base_model.encoder.layers.encoder_layer_11.mlp.4, base_model.encoder.layers.encoder_layer_11.self_attention, base_model.encoder.layers.encoder_layer_11.self_attention.out_proj, base_model.encoder.layers.encoder_layer_2, base_model.encoder.layers.encoder_layer_2.dropout, base_model.encoder.layers.encoder_layer_2.ln_1, base_model.encoder.layers.encoder_layer_2.ln_2, base_model.encoder.layers.encoder_layer_2.mlp, base_model.encoder.layers.encoder_layer_2.mlp.0, base_model.encoder.layers.encoder_layer_2.mlp.1, base_model.encoder.layers.encoder_layer_2.mlp.2, base_model.encoder.layers.encoder_layer_2.mlp.3, base_model.encoder.layers.encoder_layer_2.mlp.4, base_model.encoder.layers.encoder_layer_2.self_attention, base_model.encoder.layers.encoder_layer_2.self_attention.out_proj, base_model.encoder.layers.encoder_layer_3, base_model.encoder.layers.encoder_layer_3.dropout, base_model.encoder.layers.encoder_layer_3.ln_1, base_model.encoder.layers.encoder_layer_3.ln_2, base_model.encoder.layers.encoder_layer_3.mlp, base_model.encoder.layers.encoder_layer_3.mlp.0, base_model.encoder.layers.encoder_layer_3.mlp.1, base_model.encoder.layers.encoder_layer_3.mlp.2, base_model.encoder.layers.encoder_layer_3.mlp.3, base_model.encoder.layers.encoder_layer_3.mlp.4, base_model.encoder.layers.encoder_layer_3.self_attention, base_model.encoder.layers.encoder_layer_3.self_attention.out_proj, base_model.encoder.layers.encoder_layer_4, base_model.encoder.layers.encoder_layer_4.dropout, base_model.encoder.layers.encoder_layer_4.ln_1, base_model.encoder.layers.encoder_layer_4.ln_2, base_model.encoder.layers.encoder_layer_4.mlp, base_model.encoder.layers.encoder_layer_4.mlp.0, base_model.encoder.layers.encoder_layer_4.mlp.1, base_model.encoder.layers.encoder_layer_4.mlp.2, base_model.encoder.layers.encoder_layer_4.mlp.3, base_model.encoder.layers.encoder_layer_4.mlp.4, base_model.encoder.layers.encoder_layer_4.self_attention, base_model.encoder.layers.encoder_layer_4.self_attention.out_proj, base_model.encoder.layers.encoder_layer_5, base_model.encoder.layers.encoder_layer_5.dropout, base_model.encoder.layers.encoder_layer_5.ln_1, base_model.encoder.layers.encoder_layer_5.ln_2, base_model.encoder.layers.encoder_layer_5.mlp, base_model.encoder.layers.encoder_layer_5.mlp.0, base_model.encoder.layers.encoder_layer_5.mlp.1, base_model.encoder.layers.encoder_layer_5.mlp.2, base_model.encoder.layers.encoder_layer_5.mlp.3, base_model.encoder.layers.encoder_layer_5.mlp.4, base_model.encoder.layers.encoder_layer_5.self_attention, base_model.encoder.layers.encoder_layer_5.self_attention.out_proj, base_model.encoder.layers.encoder_layer_6, base_model.encoder.layers.encoder_layer_6.dropout, base_model.encoder.layers.encoder_layer_6.ln_1, base_model.encoder.layers.encoder_layer_6.ln_2, base_model.encoder.layers.encoder_layer_6.mlp, base_model.encoder.layers.encoder_layer_6.mlp.0, base_model.encoder.layers.encoder_layer_6.mlp.1, base_model.encoder.layers.encoder_layer_6.mlp.2, base_model.encoder.layers.encoder_layer_6.mlp.3, base_model.encoder.layers.encoder_layer_6.mlp.4, base_model.encoder.layers.encoder_layer_6.self_attention, base_model.encoder.layers.encoder_layer_6.self_attention.out_proj, base_model.encoder.layers.encoder_layer_7, base_model.encoder.layers.encoder_layer_7.dropout, base_model.encoder.layers.encoder_layer_7.ln_1, base_model.encoder.layers.encoder_layer_7.ln_2, base_model.encoder.layers.encoder_layer_7.mlp, base_model.encoder.layers.encoder_layer_7.mlp.0, base_model.encoder.layers.encoder_layer_7.mlp.1, base_model.encoder.layers.encoder_layer_7.mlp.2, base_model.encoder.layers.encoder_layer_7.mlp.3, base_model.encoder.layers.encoder_layer_7.mlp.4, base_model.encoder.layers.encoder_layer_7.self_attention, base_model.encoder.layers.encoder_layer_7.self_attention.out_proj, base_model.encoder.layers.encoder_layer_8, base_model.encoder.layers.encoder_layer_8.dropout, base_model.encoder.layers.encoder_layer_8.ln_1, base_model.encoder.layers.encoder_layer_8.ln_2, base_model.encoder.layers.encoder_layer_8.mlp, base_model.encoder.layers.encoder_layer_8.mlp.0, base_model.encoder.layers.encoder_layer_8.mlp.1, base_model.encoder.layers.encoder_layer_8.mlp.2, base_model.encoder.layers.encoder_layer_8.mlp.3, base_model.encoder.layers.encoder_layer_8.mlp.4, base_model.encoder.layers.encoder_layer_8.self_attention, base_model.encoder.layers.encoder_layer_8.self_attention.out_proj, base_model.encoder.layers.encoder_layer_9, base_model.encoder.layers.encoder_layer_9.dropout, base_model.encoder.layers.encoder_layer_9.ln_1, base_model.encoder.layers.encoder_layer_9.ln_2, base_model.encoder.layers.encoder_layer_9.mlp, base_model.encoder.layers.encoder_layer_9.mlp.0, base_model.encoder.layers.encoder_layer_9.mlp.1, base_model.encoder.layers.encoder_layer_9.mlp.2, base_model.encoder.layers.encoder_layer_9.mlp.3, base_model.encoder.layers.encoder_layer_9.mlp.4, base_model.encoder.layers.encoder_layer_9.self_attention, base_model.encoder.layers.encoder_layer_9.self_attention.out_proj, base_model.encoder.ln, base_model.heads, base_model.heads.head, classifiers.0, classifiers.2, classifiers.3, classifiers.4, classifiers.5, classifiers.6, classifiers.7, classifiers.8, classifiers.9, ees.0, ees.0.0, ees.0.0.dropout, ees.0.0.ln_1, ees.0.0.ln_2, ees.0.0.mlp, ees.0.0.mlp.0, ees.0.0.mlp.1, ees.0.0.mlp.2, ees.0.0.mlp.3, ees.0.0.mlp.4, ees.0.0.self_attention, ees.0.0.self_attention.out_proj, ees.0.1, ees.1.0.self_attention.out_proj, ees.2, ees.2.0, ees.2.0.dropout, ees.2.0.ln_1, ees.2.0.ln_2, ees.2.0.mlp, ees.2.0.mlp.0, ees.2.0.mlp.1, ees.2.0.mlp.2, ees.2.0.mlp.3, ees.2.0.mlp.4, ees.2.0.self_attention, ees.2.0.self_attention.out_proj, ees.2.1, ees.3, ees.3.0, ees.3.0.dropout, ees.3.0.ln_1, ees.3.0.ln_2, ees.3.0.mlp, ees.3.0.mlp.0, ees.3.0.mlp.1, ees.3.0.mlp.2, ees.3.0.mlp.3, ees.3.0.mlp.4, ees.3.0.self_attention, ees.3.0.self_attention.out_proj, ees.3.1, ees.4, ees.4.0, ees.4.0.dropout, ees.4.0.ln_1, ees.4.0.ln_2, ees.4.0.mlp, ees.4.0.mlp.0, ees.4.0.mlp.1, ees.4.0.mlp.2, ees.4.0.mlp.3, ees.4.0.mlp.4, ees.4.0.self_attention, ees.4.0.self_attention.out_proj, ees.4.1, ees.5, ees.5.0, ees.5.0.dropout, ees.5.0.ln_1, ees.5.0.ln_2, ees.5.0.mlp, ees.5.0.mlp.0, ees.5.0.mlp.1, ees.5.0.mlp.2, ees.5.0.mlp.3, ees.5.0.mlp.4, ees.5.0.self_attention, ees.5.0.self_attention.out_proj, ees.5.1, ees.6, ees.6.0, ees.6.0.dropout, ees.6.0.ln_1, ees.6.0.ln_2, ees.6.0.mlp, ees.6.0.mlp.0, ees.6.0.mlp.1, ees.6.0.mlp.2, ees.6.0.mlp.3, ees.6.0.mlp.4, ees.6.0.self_attention, ees.6.0.self_attention.out_proj, ees.6.1, ees.7, ees.7.0, ees.7.0.dropout, ees.7.0.ln_1, ees.7.0.ln_2, ees.7.0.mlp, ees.7.0.mlp.0, ees.7.0.mlp.1, ees.7.0.mlp.2, ees.7.0.mlp.3, ees.7.0.mlp.4, ees.7.0.self_attention, ees.7.0.self_attention.out_proj, ees.7.1, ees.8, ees.8.0, ees.8.0.dropout, ees.8.0.ln_1, ees.8.0.ln_2, ees.8.0.mlp, ees.8.0.mlp.0, ees.8.0.mlp.1, ees.8.0.mlp.2, ees.8.0.mlp.3, ees.8.0.mlp.4, ees.8.0.self_attention, ees.8.0.self_attention.out_proj, ees.8.1, ees.9, ees.9.0, ees.9.0.dropout, ees.9.0.ln_1, ees.9.0.ln_2, ees.9.0.mlp, ees.9.0.mlp.0, ees.9.0.mlp.1, ees.9.0.mlp.2, ees.9.0.mlp.3, ees.9.0.mlp.4, ees.9.0.self_attention, ees.9.0.self_attention.out_proj, ees.9.1\n",
      "Unsupported operator aten::mul encountered 17 time(s)\n",
      "Unsupported operator aten::add encountered 9 time(s)\n",
      "Unsupported operator aten::div encountered 4 time(s)\n",
      "Unsupported operator aten::unflatten encountered 4 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 4 time(s)\n",
      "Unsupported operator aten::gelu encountered 4 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "base_model.encoder.layers.encoder_layer_0.self_attention.out_proj, base_model.encoder.layers.encoder_layer_1.self_attention.out_proj, base_model.encoder.layers.encoder_layer_10, base_model.encoder.layers.encoder_layer_10.dropout, base_model.encoder.layers.encoder_layer_10.ln_1, base_model.encoder.layers.encoder_layer_10.ln_2, base_model.encoder.layers.encoder_layer_10.mlp, base_model.encoder.layers.encoder_layer_10.mlp.0, base_model.encoder.layers.encoder_layer_10.mlp.1, base_model.encoder.layers.encoder_layer_10.mlp.2, base_model.encoder.layers.encoder_layer_10.mlp.3, base_model.encoder.layers.encoder_layer_10.mlp.4, base_model.encoder.layers.encoder_layer_10.self_attention, base_model.encoder.layers.encoder_layer_10.self_attention.out_proj, base_model.encoder.layers.encoder_layer_11, base_model.encoder.layers.encoder_layer_11.dropout, base_model.encoder.layers.encoder_layer_11.ln_1, base_model.encoder.layers.encoder_layer_11.ln_2, base_model.encoder.layers.encoder_layer_11.mlp, base_model.encoder.layers.encoder_layer_11.mlp.0, base_model.encoder.layers.encoder_layer_11.mlp.1, base_model.encoder.layers.encoder_layer_11.mlp.2, base_model.encoder.layers.encoder_layer_11.mlp.3, base_model.encoder.layers.encoder_layer_11.mlp.4, base_model.encoder.layers.encoder_layer_11.self_attention, base_model.encoder.layers.encoder_layer_11.self_attention.out_proj, base_model.encoder.layers.encoder_layer_2.self_attention.out_proj, base_model.encoder.layers.encoder_layer_3, base_model.encoder.layers.encoder_layer_3.dropout, base_model.encoder.layers.encoder_layer_3.ln_1, base_model.encoder.layers.encoder_layer_3.ln_2, base_model.encoder.layers.encoder_layer_3.mlp, base_model.encoder.layers.encoder_layer_3.mlp.0, base_model.encoder.layers.encoder_layer_3.mlp.1, base_model.encoder.layers.encoder_layer_3.mlp.2, base_model.encoder.layers.encoder_layer_3.mlp.3, base_model.encoder.layers.encoder_layer_3.mlp.4, base_model.encoder.layers.encoder_layer_3.self_attention, base_model.encoder.layers.encoder_layer_3.self_attention.out_proj, base_model.encoder.layers.encoder_layer_4, base_model.encoder.layers.encoder_layer_4.dropout, base_model.encoder.layers.encoder_layer_4.ln_1, base_model.encoder.layers.encoder_layer_4.ln_2, base_model.encoder.layers.encoder_layer_4.mlp, base_model.encoder.layers.encoder_layer_4.mlp.0, base_model.encoder.layers.encoder_layer_4.mlp.1, base_model.encoder.layers.encoder_layer_4.mlp.2, base_model.encoder.layers.encoder_layer_4.mlp.3, base_model.encoder.layers.encoder_layer_4.mlp.4, base_model.encoder.layers.encoder_layer_4.self_attention, base_model.encoder.layers.encoder_layer_4.self_attention.out_proj, base_model.encoder.layers.encoder_layer_5, base_model.encoder.layers.encoder_layer_5.dropout, base_model.encoder.layers.encoder_layer_5.ln_1, base_model.encoder.layers.encoder_layer_5.ln_2, base_model.encoder.layers.encoder_layer_5.mlp, base_model.encoder.layers.encoder_layer_5.mlp.0, base_model.encoder.layers.encoder_layer_5.mlp.1, base_model.encoder.layers.encoder_layer_5.mlp.2, base_model.encoder.layers.encoder_layer_5.mlp.3, base_model.encoder.layers.encoder_layer_5.mlp.4, base_model.encoder.layers.encoder_layer_5.self_attention, base_model.encoder.layers.encoder_layer_5.self_attention.out_proj, base_model.encoder.layers.encoder_layer_6, base_model.encoder.layers.encoder_layer_6.dropout, base_model.encoder.layers.encoder_layer_6.ln_1, base_model.encoder.layers.encoder_layer_6.ln_2, base_model.encoder.layers.encoder_layer_6.mlp, base_model.encoder.layers.encoder_layer_6.mlp.0, base_model.encoder.layers.encoder_layer_6.mlp.1, base_model.encoder.layers.encoder_layer_6.mlp.2, base_model.encoder.layers.encoder_layer_6.mlp.3, base_model.encoder.layers.encoder_layer_6.mlp.4, base_model.encoder.layers.encoder_layer_6.self_attention, base_model.encoder.layers.encoder_layer_6.self_attention.out_proj, base_model.encoder.layers.encoder_layer_7, base_model.encoder.layers.encoder_layer_7.dropout, base_model.encoder.layers.encoder_layer_7.ln_1, base_model.encoder.layers.encoder_layer_7.ln_2, base_model.encoder.layers.encoder_layer_7.mlp, base_model.encoder.layers.encoder_layer_7.mlp.0, base_model.encoder.layers.encoder_layer_7.mlp.1, base_model.encoder.layers.encoder_layer_7.mlp.2, base_model.encoder.layers.encoder_layer_7.mlp.3, base_model.encoder.layers.encoder_layer_7.mlp.4, base_model.encoder.layers.encoder_layer_7.self_attention, base_model.encoder.layers.encoder_layer_7.self_attention.out_proj, base_model.encoder.layers.encoder_layer_8, base_model.encoder.layers.encoder_layer_8.dropout, base_model.encoder.layers.encoder_layer_8.ln_1, base_model.encoder.layers.encoder_layer_8.ln_2, base_model.encoder.layers.encoder_layer_8.mlp, base_model.encoder.layers.encoder_layer_8.mlp.0, base_model.encoder.layers.encoder_layer_8.mlp.1, base_model.encoder.layers.encoder_layer_8.mlp.2, base_model.encoder.layers.encoder_layer_8.mlp.3, base_model.encoder.layers.encoder_layer_8.mlp.4, base_model.encoder.layers.encoder_layer_8.self_attention, base_model.encoder.layers.encoder_layer_8.self_attention.out_proj, base_model.encoder.layers.encoder_layer_9, base_model.encoder.layers.encoder_layer_9.dropout, base_model.encoder.layers.encoder_layer_9.ln_1, base_model.encoder.layers.encoder_layer_9.ln_2, base_model.encoder.layers.encoder_layer_9.mlp, base_model.encoder.layers.encoder_layer_9.mlp.0, base_model.encoder.layers.encoder_layer_9.mlp.1, base_model.encoder.layers.encoder_layer_9.mlp.2, base_model.encoder.layers.encoder_layer_9.mlp.3, base_model.encoder.layers.encoder_layer_9.mlp.4, base_model.encoder.layers.encoder_layer_9.self_attention, base_model.encoder.layers.encoder_layer_9.self_attention.out_proj, base_model.encoder.ln, base_model.heads, base_model.heads.head, classifiers.0, classifiers.1, classifiers.3, classifiers.4, classifiers.5, classifiers.6, classifiers.7, classifiers.8, classifiers.9, ees.0, ees.0.0, ees.0.0.dropout, ees.0.0.ln_1, ees.0.0.ln_2, ees.0.0.mlp, ees.0.0.mlp.0, ees.0.0.mlp.1, ees.0.0.mlp.2, ees.0.0.mlp.3, ees.0.0.mlp.4, ees.0.0.self_attention, ees.0.0.self_attention.out_proj, ees.0.1, ees.1, ees.1.0, ees.1.0.dropout, ees.1.0.ln_1, ees.1.0.ln_2, ees.1.0.mlp, ees.1.0.mlp.0, ees.1.0.mlp.1, ees.1.0.mlp.2, ees.1.0.mlp.3, ees.1.0.mlp.4, ees.1.0.self_attention, ees.1.0.self_attention.out_proj, ees.1.1, ees.2.0.self_attention.out_proj, ees.3, ees.3.0, ees.3.0.dropout, ees.3.0.ln_1, ees.3.0.ln_2, ees.3.0.mlp, ees.3.0.mlp.0, ees.3.0.mlp.1, ees.3.0.mlp.2, ees.3.0.mlp.3, ees.3.0.mlp.4, ees.3.0.self_attention, ees.3.0.self_attention.out_proj, ees.3.1, ees.4, ees.4.0, ees.4.0.dropout, ees.4.0.ln_1, ees.4.0.ln_2, ees.4.0.mlp, ees.4.0.mlp.0, ees.4.0.mlp.1, ees.4.0.mlp.2, ees.4.0.mlp.3, ees.4.0.mlp.4, ees.4.0.self_attention, ees.4.0.self_attention.out_proj, ees.4.1, ees.5, ees.5.0, ees.5.0.dropout, ees.5.0.ln_1, ees.5.0.ln_2, ees.5.0.mlp, ees.5.0.mlp.0, ees.5.0.mlp.1, ees.5.0.mlp.2, ees.5.0.mlp.3, ees.5.0.mlp.4, ees.5.0.self_attention, ees.5.0.self_attention.out_proj, ees.5.1, ees.6, ees.6.0, ees.6.0.dropout, ees.6.0.ln_1, ees.6.0.ln_2, ees.6.0.mlp, ees.6.0.mlp.0, ees.6.0.mlp.1, ees.6.0.mlp.2, ees.6.0.mlp.3, ees.6.0.mlp.4, ees.6.0.self_attention, ees.6.0.self_attention.out_proj, ees.6.1, ees.7, ees.7.0, ees.7.0.dropout, ees.7.0.ln_1, ees.7.0.ln_2, ees.7.0.mlp, ees.7.0.mlp.0, ees.7.0.mlp.1, ees.7.0.mlp.2, ees.7.0.mlp.3, ees.7.0.mlp.4, ees.7.0.self_attention, ees.7.0.self_attention.out_proj, ees.7.1, ees.8, ees.8.0, ees.8.0.dropout, ees.8.0.ln_1, ees.8.0.ln_2, ees.8.0.mlp, ees.8.0.mlp.0, ees.8.0.mlp.1, ees.8.0.mlp.2, ees.8.0.mlp.3, ees.8.0.mlp.4, ees.8.0.self_attention, ees.8.0.self_attention.out_proj, ees.8.1, ees.9, ees.9.0, ees.9.0.dropout, ees.9.0.ln_1, ees.9.0.ln_2, ees.9.0.mlp, ees.9.0.mlp.0, ees.9.0.mlp.1, ees.9.0.mlp.2, ees.9.0.mlp.3, ees.9.0.mlp.4, ees.9.0.self_attention, ees.9.0.self_attention.out_proj, ees.9.1\n",
      "Unsupported operator aten::mul encountered 21 time(s)\n",
      "Unsupported operator aten::add encountered 11 time(s)\n",
      "Unsupported operator aten::div encountered 5 time(s)\n",
      "Unsupported operator aten::unflatten encountered 5 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 5 time(s)\n",
      "Unsupported operator aten::gelu encountered 5 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "base_model.encoder.layers.encoder_layer_0.self_attention.out_proj, base_model.encoder.layers.encoder_layer_1.self_attention.out_proj, base_model.encoder.layers.encoder_layer_10, base_model.encoder.layers.encoder_layer_10.dropout, base_model.encoder.layers.encoder_layer_10.ln_1, base_model.encoder.layers.encoder_layer_10.ln_2, base_model.encoder.layers.encoder_layer_10.mlp, base_model.encoder.layers.encoder_layer_10.mlp.0, base_model.encoder.layers.encoder_layer_10.mlp.1, base_model.encoder.layers.encoder_layer_10.mlp.2, base_model.encoder.layers.encoder_layer_10.mlp.3, base_model.encoder.layers.encoder_layer_10.mlp.4, base_model.encoder.layers.encoder_layer_10.self_attention, base_model.encoder.layers.encoder_layer_10.self_attention.out_proj, base_model.encoder.layers.encoder_layer_11, base_model.encoder.layers.encoder_layer_11.dropout, base_model.encoder.layers.encoder_layer_11.ln_1, base_model.encoder.layers.encoder_layer_11.ln_2, base_model.encoder.layers.encoder_layer_11.mlp, base_model.encoder.layers.encoder_layer_11.mlp.0, base_model.encoder.layers.encoder_layer_11.mlp.1, base_model.encoder.layers.encoder_layer_11.mlp.2, base_model.encoder.layers.encoder_layer_11.mlp.3, base_model.encoder.layers.encoder_layer_11.mlp.4, base_model.encoder.layers.encoder_layer_11.self_attention, base_model.encoder.layers.encoder_layer_11.self_attention.out_proj, base_model.encoder.layers.encoder_layer_2.self_attention.out_proj, base_model.encoder.layers.encoder_layer_3.self_attention.out_proj, base_model.encoder.layers.encoder_layer_4, base_model.encoder.layers.encoder_layer_4.dropout, base_model.encoder.layers.encoder_layer_4.ln_1, base_model.encoder.layers.encoder_layer_4.ln_2, base_model.encoder.layers.encoder_layer_4.mlp, base_model.encoder.layers.encoder_layer_4.mlp.0, base_model.encoder.layers.encoder_layer_4.mlp.1, base_model.encoder.layers.encoder_layer_4.mlp.2, base_model.encoder.layers.encoder_layer_4.mlp.3, base_model.encoder.layers.encoder_layer_4.mlp.4, base_model.encoder.layers.encoder_layer_4.self_attention, base_model.encoder.layers.encoder_layer_4.self_attention.out_proj, base_model.encoder.layers.encoder_layer_5, base_model.encoder.layers.encoder_layer_5.dropout, base_model.encoder.layers.encoder_layer_5.ln_1, base_model.encoder.layers.encoder_layer_5.ln_2, base_model.encoder.layers.encoder_layer_5.mlp, base_model.encoder.layers.encoder_layer_5.mlp.0, base_model.encoder.layers.encoder_layer_5.mlp.1, base_model.encoder.layers.encoder_layer_5.mlp.2, base_model.encoder.layers.encoder_layer_5.mlp.3, base_model.encoder.layers.encoder_layer_5.mlp.4, base_model.encoder.layers.encoder_layer_5.self_attention, base_model.encoder.layers.encoder_layer_5.self_attention.out_proj, base_model.encoder.layers.encoder_layer_6, base_model.encoder.layers.encoder_layer_6.dropout, base_model.encoder.layers.encoder_layer_6.ln_1, base_model.encoder.layers.encoder_layer_6.ln_2, base_model.encoder.layers.encoder_layer_6.mlp, base_model.encoder.layers.encoder_layer_6.mlp.0, base_model.encoder.layers.encoder_layer_6.mlp.1, base_model.encoder.layers.encoder_layer_6.mlp.2, base_model.encoder.layers.encoder_layer_6.mlp.3, base_model.encoder.layers.encoder_layer_6.mlp.4, base_model.encoder.layers.encoder_layer_6.self_attention, base_model.encoder.layers.encoder_layer_6.self_attention.out_proj, base_model.encoder.layers.encoder_layer_7, base_model.encoder.layers.encoder_layer_7.dropout, base_model.encoder.layers.encoder_layer_7.ln_1, base_model.encoder.layers.encoder_layer_7.ln_2, base_model.encoder.layers.encoder_layer_7.mlp, base_model.encoder.layers.encoder_layer_7.mlp.0, base_model.encoder.layers.encoder_layer_7.mlp.1, base_model.encoder.layers.encoder_layer_7.mlp.2, base_model.encoder.layers.encoder_layer_7.mlp.3, base_model.encoder.layers.encoder_layer_7.mlp.4, base_model.encoder.layers.encoder_layer_7.self_attention, base_model.encoder.layers.encoder_layer_7.self_attention.out_proj, base_model.encoder.layers.encoder_layer_8, base_model.encoder.layers.encoder_layer_8.dropout, base_model.encoder.layers.encoder_layer_8.ln_1, base_model.encoder.layers.encoder_layer_8.ln_2, base_model.encoder.layers.encoder_layer_8.mlp, base_model.encoder.layers.encoder_layer_8.mlp.0, base_model.encoder.layers.encoder_layer_8.mlp.1, base_model.encoder.layers.encoder_layer_8.mlp.2, base_model.encoder.layers.encoder_layer_8.mlp.3, base_model.encoder.layers.encoder_layer_8.mlp.4, base_model.encoder.layers.encoder_layer_8.self_attention, base_model.encoder.layers.encoder_layer_8.self_attention.out_proj, base_model.encoder.layers.encoder_layer_9, base_model.encoder.layers.encoder_layer_9.dropout, base_model.encoder.layers.encoder_layer_9.ln_1, base_model.encoder.layers.encoder_layer_9.ln_2, base_model.encoder.layers.encoder_layer_9.mlp, base_model.encoder.layers.encoder_layer_9.mlp.0, base_model.encoder.layers.encoder_layer_9.mlp.1, base_model.encoder.layers.encoder_layer_9.mlp.2, base_model.encoder.layers.encoder_layer_9.mlp.3, base_model.encoder.layers.encoder_layer_9.mlp.4, base_model.encoder.layers.encoder_layer_9.self_attention, base_model.encoder.layers.encoder_layer_9.self_attention.out_proj, base_model.encoder.ln, base_model.heads, base_model.heads.head, classifiers.0, classifiers.1, classifiers.2, classifiers.4, classifiers.5, classifiers.6, classifiers.7, classifiers.8, classifiers.9, ees.0, ees.0.0, ees.0.0.dropout, ees.0.0.ln_1, ees.0.0.ln_2, ees.0.0.mlp, ees.0.0.mlp.0, ees.0.0.mlp.1, ees.0.0.mlp.2, ees.0.0.mlp.3, ees.0.0.mlp.4, ees.0.0.self_attention, ees.0.0.self_attention.out_proj, ees.0.1, ees.1, ees.1.0, ees.1.0.dropout, ees.1.0.ln_1, ees.1.0.ln_2, ees.1.0.mlp, ees.1.0.mlp.0, ees.1.0.mlp.1, ees.1.0.mlp.2, ees.1.0.mlp.3, ees.1.0.mlp.4, ees.1.0.self_attention, ees.1.0.self_attention.out_proj, ees.1.1, ees.2, ees.2.0, ees.2.0.dropout, ees.2.0.ln_1, ees.2.0.ln_2, ees.2.0.mlp, ees.2.0.mlp.0, ees.2.0.mlp.1, ees.2.0.mlp.2, ees.2.0.mlp.3, ees.2.0.mlp.4, ees.2.0.self_attention, ees.2.0.self_attention.out_proj, ees.2.1, ees.3.0.self_attention.out_proj, ees.4, ees.4.0, ees.4.0.dropout, ees.4.0.ln_1, ees.4.0.ln_2, ees.4.0.mlp, ees.4.0.mlp.0, ees.4.0.mlp.1, ees.4.0.mlp.2, ees.4.0.mlp.3, ees.4.0.mlp.4, ees.4.0.self_attention, ees.4.0.self_attention.out_proj, ees.4.1, ees.5, ees.5.0, ees.5.0.dropout, ees.5.0.ln_1, ees.5.0.ln_2, ees.5.0.mlp, ees.5.0.mlp.0, ees.5.0.mlp.1, ees.5.0.mlp.2, ees.5.0.mlp.3, ees.5.0.mlp.4, ees.5.0.self_attention, ees.5.0.self_attention.out_proj, ees.5.1, ees.6, ees.6.0, ees.6.0.dropout, ees.6.0.ln_1, ees.6.0.ln_2, ees.6.0.mlp, ees.6.0.mlp.0, ees.6.0.mlp.1, ees.6.0.mlp.2, ees.6.0.mlp.3, ees.6.0.mlp.4, ees.6.0.self_attention, ees.6.0.self_attention.out_proj, ees.6.1, ees.7, ees.7.0, ees.7.0.dropout, ees.7.0.ln_1, ees.7.0.ln_2, ees.7.0.mlp, ees.7.0.mlp.0, ees.7.0.mlp.1, ees.7.0.mlp.2, ees.7.0.mlp.3, ees.7.0.mlp.4, ees.7.0.self_attention, ees.7.0.self_attention.out_proj, ees.7.1, ees.8, ees.8.0, ees.8.0.dropout, ees.8.0.ln_1, ees.8.0.ln_2, ees.8.0.mlp, ees.8.0.mlp.0, ees.8.0.mlp.1, ees.8.0.mlp.2, ees.8.0.mlp.3, ees.8.0.mlp.4, ees.8.0.self_attention, ees.8.0.self_attention.out_proj, ees.8.1, ees.9, ees.9.0, ees.9.0.dropout, ees.9.0.ln_1, ees.9.0.ln_2, ees.9.0.mlp, ees.9.0.mlp.0, ees.9.0.mlp.1, ees.9.0.mlp.2, ees.9.0.mlp.3, ees.9.0.mlp.4, ees.9.0.self_attention, ees.9.0.self_attention.out_proj, ees.9.1\n",
      "Unsupported operator aten::mul encountered 25 time(s)\n",
      "Unsupported operator aten::add encountered 13 time(s)\n",
      "Unsupported operator aten::div encountered 6 time(s)\n",
      "Unsupported operator aten::unflatten encountered 6 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 6 time(s)\n",
      "Unsupported operator aten::gelu encountered 6 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "base_model.encoder.layers.encoder_layer_0.self_attention.out_proj, base_model.encoder.layers.encoder_layer_1.self_attention.out_proj, base_model.encoder.layers.encoder_layer_10, base_model.encoder.layers.encoder_layer_10.dropout, base_model.encoder.layers.encoder_layer_10.ln_1, base_model.encoder.layers.encoder_layer_10.ln_2, base_model.encoder.layers.encoder_layer_10.mlp, base_model.encoder.layers.encoder_layer_10.mlp.0, base_model.encoder.layers.encoder_layer_10.mlp.1, base_model.encoder.layers.encoder_layer_10.mlp.2, base_model.encoder.layers.encoder_layer_10.mlp.3, base_model.encoder.layers.encoder_layer_10.mlp.4, base_model.encoder.layers.encoder_layer_10.self_attention, base_model.encoder.layers.encoder_layer_10.self_attention.out_proj, base_model.encoder.layers.encoder_layer_11, base_model.encoder.layers.encoder_layer_11.dropout, base_model.encoder.layers.encoder_layer_11.ln_1, base_model.encoder.layers.encoder_layer_11.ln_2, base_model.encoder.layers.encoder_layer_11.mlp, base_model.encoder.layers.encoder_layer_11.mlp.0, base_model.encoder.layers.encoder_layer_11.mlp.1, base_model.encoder.layers.encoder_layer_11.mlp.2, base_model.encoder.layers.encoder_layer_11.mlp.3, base_model.encoder.layers.encoder_layer_11.mlp.4, base_model.encoder.layers.encoder_layer_11.self_attention, base_model.encoder.layers.encoder_layer_11.self_attention.out_proj, base_model.encoder.layers.encoder_layer_2.self_attention.out_proj, base_model.encoder.layers.encoder_layer_3.self_attention.out_proj, base_model.encoder.layers.encoder_layer_4.self_attention.out_proj, base_model.encoder.layers.encoder_layer_5, base_model.encoder.layers.encoder_layer_5.dropout, base_model.encoder.layers.encoder_layer_5.ln_1, base_model.encoder.layers.encoder_layer_5.ln_2, base_model.encoder.layers.encoder_layer_5.mlp, base_model.encoder.layers.encoder_layer_5.mlp.0, base_model.encoder.layers.encoder_layer_5.mlp.1, base_model.encoder.layers.encoder_layer_5.mlp.2, base_model.encoder.layers.encoder_layer_5.mlp.3, base_model.encoder.layers.encoder_layer_5.mlp.4, base_model.encoder.layers.encoder_layer_5.self_attention, base_model.encoder.layers.encoder_layer_5.self_attention.out_proj, base_model.encoder.layers.encoder_layer_6, base_model.encoder.layers.encoder_layer_6.dropout, base_model.encoder.layers.encoder_layer_6.ln_1, base_model.encoder.layers.encoder_layer_6.ln_2, base_model.encoder.layers.encoder_layer_6.mlp, base_model.encoder.layers.encoder_layer_6.mlp.0, base_model.encoder.layers.encoder_layer_6.mlp.1, base_model.encoder.layers.encoder_layer_6.mlp.2, base_model.encoder.layers.encoder_layer_6.mlp.3, base_model.encoder.layers.encoder_layer_6.mlp.4, base_model.encoder.layers.encoder_layer_6.self_attention, base_model.encoder.layers.encoder_layer_6.self_attention.out_proj, base_model.encoder.layers.encoder_layer_7, base_model.encoder.layers.encoder_layer_7.dropout, base_model.encoder.layers.encoder_layer_7.ln_1, base_model.encoder.layers.encoder_layer_7.ln_2, base_model.encoder.layers.encoder_layer_7.mlp, base_model.encoder.layers.encoder_layer_7.mlp.0, base_model.encoder.layers.encoder_layer_7.mlp.1, base_model.encoder.layers.encoder_layer_7.mlp.2, base_model.encoder.layers.encoder_layer_7.mlp.3, base_model.encoder.layers.encoder_layer_7.mlp.4, base_model.encoder.layers.encoder_layer_7.self_attention, base_model.encoder.layers.encoder_layer_7.self_attention.out_proj, base_model.encoder.layers.encoder_layer_8, base_model.encoder.layers.encoder_layer_8.dropout, base_model.encoder.layers.encoder_layer_8.ln_1, base_model.encoder.layers.encoder_layer_8.ln_2, base_model.encoder.layers.encoder_layer_8.mlp, base_model.encoder.layers.encoder_layer_8.mlp.0, base_model.encoder.layers.encoder_layer_8.mlp.1, base_model.encoder.layers.encoder_layer_8.mlp.2, base_model.encoder.layers.encoder_layer_8.mlp.3, base_model.encoder.layers.encoder_layer_8.mlp.4, base_model.encoder.layers.encoder_layer_8.self_attention, base_model.encoder.layers.encoder_layer_8.self_attention.out_proj, base_model.encoder.layers.encoder_layer_9, base_model.encoder.layers.encoder_layer_9.dropout, base_model.encoder.layers.encoder_layer_9.ln_1, base_model.encoder.layers.encoder_layer_9.ln_2, base_model.encoder.layers.encoder_layer_9.mlp, base_model.encoder.layers.encoder_layer_9.mlp.0, base_model.encoder.layers.encoder_layer_9.mlp.1, base_model.encoder.layers.encoder_layer_9.mlp.2, base_model.encoder.layers.encoder_layer_9.mlp.3, base_model.encoder.layers.encoder_layer_9.mlp.4, base_model.encoder.layers.encoder_layer_9.self_attention, base_model.encoder.layers.encoder_layer_9.self_attention.out_proj, base_model.encoder.ln, base_model.heads, base_model.heads.head, classifiers.0, classifiers.1, classifiers.2, classifiers.3, classifiers.5, classifiers.6, classifiers.7, classifiers.8, classifiers.9, ees.0, ees.0.0, ees.0.0.dropout, ees.0.0.ln_1, ees.0.0.ln_2, ees.0.0.mlp, ees.0.0.mlp.0, ees.0.0.mlp.1, ees.0.0.mlp.2, ees.0.0.mlp.3, ees.0.0.mlp.4, ees.0.0.self_attention, ees.0.0.self_attention.out_proj, ees.0.1, ees.1, ees.1.0, ees.1.0.dropout, ees.1.0.ln_1, ees.1.0.ln_2, ees.1.0.mlp, ees.1.0.mlp.0, ees.1.0.mlp.1, ees.1.0.mlp.2, ees.1.0.mlp.3, ees.1.0.mlp.4, ees.1.0.self_attention, ees.1.0.self_attention.out_proj, ees.1.1, ees.2, ees.2.0, ees.2.0.dropout, ees.2.0.ln_1, ees.2.0.ln_2, ees.2.0.mlp, ees.2.0.mlp.0, ees.2.0.mlp.1, ees.2.0.mlp.2, ees.2.0.mlp.3, ees.2.0.mlp.4, ees.2.0.self_attention, ees.2.0.self_attention.out_proj, ees.2.1, ees.3, ees.3.0, ees.3.0.dropout, ees.3.0.ln_1, ees.3.0.ln_2, ees.3.0.mlp, ees.3.0.mlp.0, ees.3.0.mlp.1, ees.3.0.mlp.2, ees.3.0.mlp.3, ees.3.0.mlp.4, ees.3.0.self_attention, ees.3.0.self_attention.out_proj, ees.3.1, ees.4.0.self_attention.out_proj, ees.5, ees.5.0, ees.5.0.dropout, ees.5.0.ln_1, ees.5.0.ln_2, ees.5.0.mlp, ees.5.0.mlp.0, ees.5.0.mlp.1, ees.5.0.mlp.2, ees.5.0.mlp.3, ees.5.0.mlp.4, ees.5.0.self_attention, ees.5.0.self_attention.out_proj, ees.5.1, ees.6, ees.6.0, ees.6.0.dropout, ees.6.0.ln_1, ees.6.0.ln_2, ees.6.0.mlp, ees.6.0.mlp.0, ees.6.0.mlp.1, ees.6.0.mlp.2, ees.6.0.mlp.3, ees.6.0.mlp.4, ees.6.0.self_attention, ees.6.0.self_attention.out_proj, ees.6.1, ees.7, ees.7.0, ees.7.0.dropout, ees.7.0.ln_1, ees.7.0.ln_2, ees.7.0.mlp, ees.7.0.mlp.0, ees.7.0.mlp.1, ees.7.0.mlp.2, ees.7.0.mlp.3, ees.7.0.mlp.4, ees.7.0.self_attention, ees.7.0.self_attention.out_proj, ees.7.1, ees.8, ees.8.0, ees.8.0.dropout, ees.8.0.ln_1, ees.8.0.ln_2, ees.8.0.mlp, ees.8.0.mlp.0, ees.8.0.mlp.1, ees.8.0.mlp.2, ees.8.0.mlp.3, ees.8.0.mlp.4, ees.8.0.self_attention, ees.8.0.self_attention.out_proj, ees.8.1, ees.9, ees.9.0, ees.9.0.dropout, ees.9.0.ln_1, ees.9.0.ln_2, ees.9.0.mlp, ees.9.0.mlp.0, ees.9.0.mlp.1, ees.9.0.mlp.2, ees.9.0.mlp.3, ees.9.0.mlp.4, ees.9.0.self_attention, ees.9.0.self_attention.out_proj, ees.9.1\n",
      "Unsupported operator aten::mul encountered 29 time(s)\n",
      "Unsupported operator aten::add encountered 15 time(s)\n",
      "Unsupported operator aten::div encountered 7 time(s)\n",
      "Unsupported operator aten::unflatten encountered 7 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 7 time(s)\n",
      "Unsupported operator aten::gelu encountered 7 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "base_model.encoder.layers.encoder_layer_0.self_attention.out_proj, base_model.encoder.layers.encoder_layer_1.self_attention.out_proj, base_model.encoder.layers.encoder_layer_10, base_model.encoder.layers.encoder_layer_10.dropout, base_model.encoder.layers.encoder_layer_10.ln_1, base_model.encoder.layers.encoder_layer_10.ln_2, base_model.encoder.layers.encoder_layer_10.mlp, base_model.encoder.layers.encoder_layer_10.mlp.0, base_model.encoder.layers.encoder_layer_10.mlp.1, base_model.encoder.layers.encoder_layer_10.mlp.2, base_model.encoder.layers.encoder_layer_10.mlp.3, base_model.encoder.layers.encoder_layer_10.mlp.4, base_model.encoder.layers.encoder_layer_10.self_attention, base_model.encoder.layers.encoder_layer_10.self_attention.out_proj, base_model.encoder.layers.encoder_layer_11, base_model.encoder.layers.encoder_layer_11.dropout, base_model.encoder.layers.encoder_layer_11.ln_1, base_model.encoder.layers.encoder_layer_11.ln_2, base_model.encoder.layers.encoder_layer_11.mlp, base_model.encoder.layers.encoder_layer_11.mlp.0, base_model.encoder.layers.encoder_layer_11.mlp.1, base_model.encoder.layers.encoder_layer_11.mlp.2, base_model.encoder.layers.encoder_layer_11.mlp.3, base_model.encoder.layers.encoder_layer_11.mlp.4, base_model.encoder.layers.encoder_layer_11.self_attention, base_model.encoder.layers.encoder_layer_11.self_attention.out_proj, base_model.encoder.layers.encoder_layer_2.self_attention.out_proj, base_model.encoder.layers.encoder_layer_3.self_attention.out_proj, base_model.encoder.layers.encoder_layer_4.self_attention.out_proj, base_model.encoder.layers.encoder_layer_5.self_attention.out_proj, base_model.encoder.layers.encoder_layer_6, base_model.encoder.layers.encoder_layer_6.dropout, base_model.encoder.layers.encoder_layer_6.ln_1, base_model.encoder.layers.encoder_layer_6.ln_2, base_model.encoder.layers.encoder_layer_6.mlp, base_model.encoder.layers.encoder_layer_6.mlp.0, base_model.encoder.layers.encoder_layer_6.mlp.1, base_model.encoder.layers.encoder_layer_6.mlp.2, base_model.encoder.layers.encoder_layer_6.mlp.3, base_model.encoder.layers.encoder_layer_6.mlp.4, base_model.encoder.layers.encoder_layer_6.self_attention, base_model.encoder.layers.encoder_layer_6.self_attention.out_proj, base_model.encoder.layers.encoder_layer_7, base_model.encoder.layers.encoder_layer_7.dropout, base_model.encoder.layers.encoder_layer_7.ln_1, base_model.encoder.layers.encoder_layer_7.ln_2, base_model.encoder.layers.encoder_layer_7.mlp, base_model.encoder.layers.encoder_layer_7.mlp.0, base_model.encoder.layers.encoder_layer_7.mlp.1, base_model.encoder.layers.encoder_layer_7.mlp.2, base_model.encoder.layers.encoder_layer_7.mlp.3, base_model.encoder.layers.encoder_layer_7.mlp.4, base_model.encoder.layers.encoder_layer_7.self_attention, base_model.encoder.layers.encoder_layer_7.self_attention.out_proj, base_model.encoder.layers.encoder_layer_8, base_model.encoder.layers.encoder_layer_8.dropout, base_model.encoder.layers.encoder_layer_8.ln_1, base_model.encoder.layers.encoder_layer_8.ln_2, base_model.encoder.layers.encoder_layer_8.mlp, base_model.encoder.layers.encoder_layer_8.mlp.0, base_model.encoder.layers.encoder_layer_8.mlp.1, base_model.encoder.layers.encoder_layer_8.mlp.2, base_model.encoder.layers.encoder_layer_8.mlp.3, base_model.encoder.layers.encoder_layer_8.mlp.4, base_model.encoder.layers.encoder_layer_8.self_attention, base_model.encoder.layers.encoder_layer_8.self_attention.out_proj, base_model.encoder.layers.encoder_layer_9, base_model.encoder.layers.encoder_layer_9.dropout, base_model.encoder.layers.encoder_layer_9.ln_1, base_model.encoder.layers.encoder_layer_9.ln_2, base_model.encoder.layers.encoder_layer_9.mlp, base_model.encoder.layers.encoder_layer_9.mlp.0, base_model.encoder.layers.encoder_layer_9.mlp.1, base_model.encoder.layers.encoder_layer_9.mlp.2, base_model.encoder.layers.encoder_layer_9.mlp.3, base_model.encoder.layers.encoder_layer_9.mlp.4, base_model.encoder.layers.encoder_layer_9.self_attention, base_model.encoder.layers.encoder_layer_9.self_attention.out_proj, base_model.encoder.ln, base_model.heads, base_model.heads.head, classifiers.0, classifiers.1, classifiers.2, classifiers.3, classifiers.4, classifiers.6, classifiers.7, classifiers.8, classifiers.9, ees.0, ees.0.0, ees.0.0.dropout, ees.0.0.ln_1, ees.0.0.ln_2, ees.0.0.mlp, ees.0.0.mlp.0, ees.0.0.mlp.1, ees.0.0.mlp.2, ees.0.0.mlp.3, ees.0.0.mlp.4, ees.0.0.self_attention, ees.0.0.self_attention.out_proj, ees.0.1, ees.1, ees.1.0, ees.1.0.dropout, ees.1.0.ln_1, ees.1.0.ln_2, ees.1.0.mlp, ees.1.0.mlp.0, ees.1.0.mlp.1, ees.1.0.mlp.2, ees.1.0.mlp.3, ees.1.0.mlp.4, ees.1.0.self_attention, ees.1.0.self_attention.out_proj, ees.1.1, ees.2, ees.2.0, ees.2.0.dropout, ees.2.0.ln_1, ees.2.0.ln_2, ees.2.0.mlp, ees.2.0.mlp.0, ees.2.0.mlp.1, ees.2.0.mlp.2, ees.2.0.mlp.3, ees.2.0.mlp.4, ees.2.0.self_attention, ees.2.0.self_attention.out_proj, ees.2.1, ees.3, ees.3.0, ees.3.0.dropout, ees.3.0.ln_1, ees.3.0.ln_2, ees.3.0.mlp, ees.3.0.mlp.0, ees.3.0.mlp.1, ees.3.0.mlp.2, ees.3.0.mlp.3, ees.3.0.mlp.4, ees.3.0.self_attention, ees.3.0.self_attention.out_proj, ees.3.1, ees.4, ees.4.0, ees.4.0.dropout, ees.4.0.ln_1, ees.4.0.ln_2, ees.4.0.mlp, ees.4.0.mlp.0, ees.4.0.mlp.1, ees.4.0.mlp.2, ees.4.0.mlp.3, ees.4.0.mlp.4, ees.4.0.self_attention, ees.4.0.self_attention.out_proj, ees.4.1, ees.5.0.self_attention.out_proj, ees.6, ees.6.0, ees.6.0.dropout, ees.6.0.ln_1, ees.6.0.ln_2, ees.6.0.mlp, ees.6.0.mlp.0, ees.6.0.mlp.1, ees.6.0.mlp.2, ees.6.0.mlp.3, ees.6.0.mlp.4, ees.6.0.self_attention, ees.6.0.self_attention.out_proj, ees.6.1, ees.7, ees.7.0, ees.7.0.dropout, ees.7.0.ln_1, ees.7.0.ln_2, ees.7.0.mlp, ees.7.0.mlp.0, ees.7.0.mlp.1, ees.7.0.mlp.2, ees.7.0.mlp.3, ees.7.0.mlp.4, ees.7.0.self_attention, ees.7.0.self_attention.out_proj, ees.7.1, ees.8, ees.8.0, ees.8.0.dropout, ees.8.0.ln_1, ees.8.0.ln_2, ees.8.0.mlp, ees.8.0.mlp.0, ees.8.0.mlp.1, ees.8.0.mlp.2, ees.8.0.mlp.3, ees.8.0.mlp.4, ees.8.0.self_attention, ees.8.0.self_attention.out_proj, ees.8.1, ees.9, ees.9.0, ees.9.0.dropout, ees.9.0.ln_1, ees.9.0.ln_2, ees.9.0.mlp, ees.9.0.mlp.0, ees.9.0.mlp.1, ees.9.0.mlp.2, ees.9.0.mlp.3, ees.9.0.mlp.4, ees.9.0.self_attention, ees.9.0.self_attention.out_proj, ees.9.1\n",
      "Unsupported operator aten::mul encountered 33 time(s)\n",
      "Unsupported operator aten::add encountered 17 time(s)\n",
      "Unsupported operator aten::div encountered 8 time(s)\n",
      "Unsupported operator aten::unflatten encountered 8 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 8 time(s)\n",
      "Unsupported operator aten::gelu encountered 8 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "base_model.encoder.layers.encoder_layer_0.self_attention.out_proj, base_model.encoder.layers.encoder_layer_1.self_attention.out_proj, base_model.encoder.layers.encoder_layer_10, base_model.encoder.layers.encoder_layer_10.dropout, base_model.encoder.layers.encoder_layer_10.ln_1, base_model.encoder.layers.encoder_layer_10.ln_2, base_model.encoder.layers.encoder_layer_10.mlp, base_model.encoder.layers.encoder_layer_10.mlp.0, base_model.encoder.layers.encoder_layer_10.mlp.1, base_model.encoder.layers.encoder_layer_10.mlp.2, base_model.encoder.layers.encoder_layer_10.mlp.3, base_model.encoder.layers.encoder_layer_10.mlp.4, base_model.encoder.layers.encoder_layer_10.self_attention, base_model.encoder.layers.encoder_layer_10.self_attention.out_proj, base_model.encoder.layers.encoder_layer_11, base_model.encoder.layers.encoder_layer_11.dropout, base_model.encoder.layers.encoder_layer_11.ln_1, base_model.encoder.layers.encoder_layer_11.ln_2, base_model.encoder.layers.encoder_layer_11.mlp, base_model.encoder.layers.encoder_layer_11.mlp.0, base_model.encoder.layers.encoder_layer_11.mlp.1, base_model.encoder.layers.encoder_layer_11.mlp.2, base_model.encoder.layers.encoder_layer_11.mlp.3, base_model.encoder.layers.encoder_layer_11.mlp.4, base_model.encoder.layers.encoder_layer_11.self_attention, base_model.encoder.layers.encoder_layer_11.self_attention.out_proj, base_model.encoder.layers.encoder_layer_2.self_attention.out_proj, base_model.encoder.layers.encoder_layer_3.self_attention.out_proj, base_model.encoder.layers.encoder_layer_4.self_attention.out_proj, base_model.encoder.layers.encoder_layer_5.self_attention.out_proj, base_model.encoder.layers.encoder_layer_6.self_attention.out_proj, base_model.encoder.layers.encoder_layer_7, base_model.encoder.layers.encoder_layer_7.dropout, base_model.encoder.layers.encoder_layer_7.ln_1, base_model.encoder.layers.encoder_layer_7.ln_2, base_model.encoder.layers.encoder_layer_7.mlp, base_model.encoder.layers.encoder_layer_7.mlp.0, base_model.encoder.layers.encoder_layer_7.mlp.1, base_model.encoder.layers.encoder_layer_7.mlp.2, base_model.encoder.layers.encoder_layer_7.mlp.3, base_model.encoder.layers.encoder_layer_7.mlp.4, base_model.encoder.layers.encoder_layer_7.self_attention, base_model.encoder.layers.encoder_layer_7.self_attention.out_proj, base_model.encoder.layers.encoder_layer_8, base_model.encoder.layers.encoder_layer_8.dropout, base_model.encoder.layers.encoder_layer_8.ln_1, base_model.encoder.layers.encoder_layer_8.ln_2, base_model.encoder.layers.encoder_layer_8.mlp, base_model.encoder.layers.encoder_layer_8.mlp.0, base_model.encoder.layers.encoder_layer_8.mlp.1, base_model.encoder.layers.encoder_layer_8.mlp.2, base_model.encoder.layers.encoder_layer_8.mlp.3, base_model.encoder.layers.encoder_layer_8.mlp.4, base_model.encoder.layers.encoder_layer_8.self_attention, base_model.encoder.layers.encoder_layer_8.self_attention.out_proj, base_model.encoder.layers.encoder_layer_9, base_model.encoder.layers.encoder_layer_9.dropout, base_model.encoder.layers.encoder_layer_9.ln_1, base_model.encoder.layers.encoder_layer_9.ln_2, base_model.encoder.layers.encoder_layer_9.mlp, base_model.encoder.layers.encoder_layer_9.mlp.0, base_model.encoder.layers.encoder_layer_9.mlp.1, base_model.encoder.layers.encoder_layer_9.mlp.2, base_model.encoder.layers.encoder_layer_9.mlp.3, base_model.encoder.layers.encoder_layer_9.mlp.4, base_model.encoder.layers.encoder_layer_9.self_attention, base_model.encoder.layers.encoder_layer_9.self_attention.out_proj, base_model.encoder.ln, base_model.heads, base_model.heads.head, classifiers.0, classifiers.1, classifiers.2, classifiers.3, classifiers.4, classifiers.5, classifiers.7, classifiers.8, classifiers.9, ees.0, ees.0.0, ees.0.0.dropout, ees.0.0.ln_1, ees.0.0.ln_2, ees.0.0.mlp, ees.0.0.mlp.0, ees.0.0.mlp.1, ees.0.0.mlp.2, ees.0.0.mlp.3, ees.0.0.mlp.4, ees.0.0.self_attention, ees.0.0.self_attention.out_proj, ees.0.1, ees.1, ees.1.0, ees.1.0.dropout, ees.1.0.ln_1, ees.1.0.ln_2, ees.1.0.mlp, ees.1.0.mlp.0, ees.1.0.mlp.1, ees.1.0.mlp.2, ees.1.0.mlp.3, ees.1.0.mlp.4, ees.1.0.self_attention, ees.1.0.self_attention.out_proj, ees.1.1, ees.2, ees.2.0, ees.2.0.dropout, ees.2.0.ln_1, ees.2.0.ln_2, ees.2.0.mlp, ees.2.0.mlp.0, ees.2.0.mlp.1, ees.2.0.mlp.2, ees.2.0.mlp.3, ees.2.0.mlp.4, ees.2.0.self_attention, ees.2.0.self_attention.out_proj, ees.2.1, ees.3, ees.3.0, ees.3.0.dropout, ees.3.0.ln_1, ees.3.0.ln_2, ees.3.0.mlp, ees.3.0.mlp.0, ees.3.0.mlp.1, ees.3.0.mlp.2, ees.3.0.mlp.3, ees.3.0.mlp.4, ees.3.0.self_attention, ees.3.0.self_attention.out_proj, ees.3.1, ees.4, ees.4.0, ees.4.0.dropout, ees.4.0.ln_1, ees.4.0.ln_2, ees.4.0.mlp, ees.4.0.mlp.0, ees.4.0.mlp.1, ees.4.0.mlp.2, ees.4.0.mlp.3, ees.4.0.mlp.4, ees.4.0.self_attention, ees.4.0.self_attention.out_proj, ees.4.1, ees.5, ees.5.0, ees.5.0.dropout, ees.5.0.ln_1, ees.5.0.ln_2, ees.5.0.mlp, ees.5.0.mlp.0, ees.5.0.mlp.1, ees.5.0.mlp.2, ees.5.0.mlp.3, ees.5.0.mlp.4, ees.5.0.self_attention, ees.5.0.self_attention.out_proj, ees.5.1, ees.6.0.self_attention.out_proj, ees.7, ees.7.0, ees.7.0.dropout, ees.7.0.ln_1, ees.7.0.ln_2, ees.7.0.mlp, ees.7.0.mlp.0, ees.7.0.mlp.1, ees.7.0.mlp.2, ees.7.0.mlp.3, ees.7.0.mlp.4, ees.7.0.self_attention, ees.7.0.self_attention.out_proj, ees.7.1, ees.8, ees.8.0, ees.8.0.dropout, ees.8.0.ln_1, ees.8.0.ln_2, ees.8.0.mlp, ees.8.0.mlp.0, ees.8.0.mlp.1, ees.8.0.mlp.2, ees.8.0.mlp.3, ees.8.0.mlp.4, ees.8.0.self_attention, ees.8.0.self_attention.out_proj, ees.8.1, ees.9, ees.9.0, ees.9.0.dropout, ees.9.0.ln_1, ees.9.0.ln_2, ees.9.0.mlp, ees.9.0.mlp.0, ees.9.0.mlp.1, ees.9.0.mlp.2, ees.9.0.mlp.3, ees.9.0.mlp.4, ees.9.0.self_attention, ees.9.0.self_attention.out_proj, ees.9.1\n",
      "Unsupported operator aten::mul encountered 37 time(s)\n",
      "Unsupported operator aten::add encountered 19 time(s)\n",
      "Unsupported operator aten::div encountered 9 time(s)\n",
      "Unsupported operator aten::unflatten encountered 9 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 9 time(s)\n",
      "Unsupported operator aten::gelu encountered 9 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "base_model.encoder.layers.encoder_layer_0.self_attention.out_proj, base_model.encoder.layers.encoder_layer_1.self_attention.out_proj, base_model.encoder.layers.encoder_layer_10, base_model.encoder.layers.encoder_layer_10.dropout, base_model.encoder.layers.encoder_layer_10.ln_1, base_model.encoder.layers.encoder_layer_10.ln_2, base_model.encoder.layers.encoder_layer_10.mlp, base_model.encoder.layers.encoder_layer_10.mlp.0, base_model.encoder.layers.encoder_layer_10.mlp.1, base_model.encoder.layers.encoder_layer_10.mlp.2, base_model.encoder.layers.encoder_layer_10.mlp.3, base_model.encoder.layers.encoder_layer_10.mlp.4, base_model.encoder.layers.encoder_layer_10.self_attention, base_model.encoder.layers.encoder_layer_10.self_attention.out_proj, base_model.encoder.layers.encoder_layer_11, base_model.encoder.layers.encoder_layer_11.dropout, base_model.encoder.layers.encoder_layer_11.ln_1, base_model.encoder.layers.encoder_layer_11.ln_2, base_model.encoder.layers.encoder_layer_11.mlp, base_model.encoder.layers.encoder_layer_11.mlp.0, base_model.encoder.layers.encoder_layer_11.mlp.1, base_model.encoder.layers.encoder_layer_11.mlp.2, base_model.encoder.layers.encoder_layer_11.mlp.3, base_model.encoder.layers.encoder_layer_11.mlp.4, base_model.encoder.layers.encoder_layer_11.self_attention, base_model.encoder.layers.encoder_layer_11.self_attention.out_proj, base_model.encoder.layers.encoder_layer_2.self_attention.out_proj, base_model.encoder.layers.encoder_layer_3.self_attention.out_proj, base_model.encoder.layers.encoder_layer_4.self_attention.out_proj, base_model.encoder.layers.encoder_layer_5.self_attention.out_proj, base_model.encoder.layers.encoder_layer_6.self_attention.out_proj, base_model.encoder.layers.encoder_layer_7.self_attention.out_proj, base_model.encoder.layers.encoder_layer_8, base_model.encoder.layers.encoder_layer_8.dropout, base_model.encoder.layers.encoder_layer_8.ln_1, base_model.encoder.layers.encoder_layer_8.ln_2, base_model.encoder.layers.encoder_layer_8.mlp, base_model.encoder.layers.encoder_layer_8.mlp.0, base_model.encoder.layers.encoder_layer_8.mlp.1, base_model.encoder.layers.encoder_layer_8.mlp.2, base_model.encoder.layers.encoder_layer_8.mlp.3, base_model.encoder.layers.encoder_layer_8.mlp.4, base_model.encoder.layers.encoder_layer_8.self_attention, base_model.encoder.layers.encoder_layer_8.self_attention.out_proj, base_model.encoder.layers.encoder_layer_9, base_model.encoder.layers.encoder_layer_9.dropout, base_model.encoder.layers.encoder_layer_9.ln_1, base_model.encoder.layers.encoder_layer_9.ln_2, base_model.encoder.layers.encoder_layer_9.mlp, base_model.encoder.layers.encoder_layer_9.mlp.0, base_model.encoder.layers.encoder_layer_9.mlp.1, base_model.encoder.layers.encoder_layer_9.mlp.2, base_model.encoder.layers.encoder_layer_9.mlp.3, base_model.encoder.layers.encoder_layer_9.mlp.4, base_model.encoder.layers.encoder_layer_9.self_attention, base_model.encoder.layers.encoder_layer_9.self_attention.out_proj, base_model.encoder.ln, base_model.heads, base_model.heads.head, classifiers.0, classifiers.1, classifiers.2, classifiers.3, classifiers.4, classifiers.5, classifiers.6, classifiers.8, classifiers.9, ees.0, ees.0.0, ees.0.0.dropout, ees.0.0.ln_1, ees.0.0.ln_2, ees.0.0.mlp, ees.0.0.mlp.0, ees.0.0.mlp.1, ees.0.0.mlp.2, ees.0.0.mlp.3, ees.0.0.mlp.4, ees.0.0.self_attention, ees.0.0.self_attention.out_proj, ees.0.1, ees.1, ees.1.0, ees.1.0.dropout, ees.1.0.ln_1, ees.1.0.ln_2, ees.1.0.mlp, ees.1.0.mlp.0, ees.1.0.mlp.1, ees.1.0.mlp.2, ees.1.0.mlp.3, ees.1.0.mlp.4, ees.1.0.self_attention, ees.1.0.self_attention.out_proj, ees.1.1, ees.2, ees.2.0, ees.2.0.dropout, ees.2.0.ln_1, ees.2.0.ln_2, ees.2.0.mlp, ees.2.0.mlp.0, ees.2.0.mlp.1, ees.2.0.mlp.2, ees.2.0.mlp.3, ees.2.0.mlp.4, ees.2.0.self_attention, ees.2.0.self_attention.out_proj, ees.2.1, ees.3, ees.3.0, ees.3.0.dropout, ees.3.0.ln_1, ees.3.0.ln_2, ees.3.0.mlp, ees.3.0.mlp.0, ees.3.0.mlp.1, ees.3.0.mlp.2, ees.3.0.mlp.3, ees.3.0.mlp.4, ees.3.0.self_attention, ees.3.0.self_attention.out_proj, ees.3.1, ees.4, ees.4.0, ees.4.0.dropout, ees.4.0.ln_1, ees.4.0.ln_2, ees.4.0.mlp, ees.4.0.mlp.0, ees.4.0.mlp.1, ees.4.0.mlp.2, ees.4.0.mlp.3, ees.4.0.mlp.4, ees.4.0.self_attention, ees.4.0.self_attention.out_proj, ees.4.1, ees.5, ees.5.0, ees.5.0.dropout, ees.5.0.ln_1, ees.5.0.ln_2, ees.5.0.mlp, ees.5.0.mlp.0, ees.5.0.mlp.1, ees.5.0.mlp.2, ees.5.0.mlp.3, ees.5.0.mlp.4, ees.5.0.self_attention, ees.5.0.self_attention.out_proj, ees.5.1, ees.6, ees.6.0, ees.6.0.dropout, ees.6.0.ln_1, ees.6.0.ln_2, ees.6.0.mlp, ees.6.0.mlp.0, ees.6.0.mlp.1, ees.6.0.mlp.2, ees.6.0.mlp.3, ees.6.0.mlp.4, ees.6.0.self_attention, ees.6.0.self_attention.out_proj, ees.6.1, ees.7.0.self_attention.out_proj, ees.8, ees.8.0, ees.8.0.dropout, ees.8.0.ln_1, ees.8.0.ln_2, ees.8.0.mlp, ees.8.0.mlp.0, ees.8.0.mlp.1, ees.8.0.mlp.2, ees.8.0.mlp.3, ees.8.0.mlp.4, ees.8.0.self_attention, ees.8.0.self_attention.out_proj, ees.8.1, ees.9, ees.9.0, ees.9.0.dropout, ees.9.0.ln_1, ees.9.0.ln_2, ees.9.0.mlp, ees.9.0.mlp.0, ees.9.0.mlp.1, ees.9.0.mlp.2, ees.9.0.mlp.3, ees.9.0.mlp.4, ees.9.0.self_attention, ees.9.0.self_attention.out_proj, ees.9.1\n",
      "Unsupported operator aten::mul encountered 41 time(s)\n",
      "Unsupported operator aten::add encountered 21 time(s)\n",
      "Unsupported operator aten::div encountered 10 time(s)\n",
      "Unsupported operator aten::unflatten encountered 10 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 10 time(s)\n",
      "Unsupported operator aten::gelu encountered 10 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "base_model.encoder.layers.encoder_layer_0.self_attention.out_proj, base_model.encoder.layers.encoder_layer_1.self_attention.out_proj, base_model.encoder.layers.encoder_layer_10, base_model.encoder.layers.encoder_layer_10.dropout, base_model.encoder.layers.encoder_layer_10.ln_1, base_model.encoder.layers.encoder_layer_10.ln_2, base_model.encoder.layers.encoder_layer_10.mlp, base_model.encoder.layers.encoder_layer_10.mlp.0, base_model.encoder.layers.encoder_layer_10.mlp.1, base_model.encoder.layers.encoder_layer_10.mlp.2, base_model.encoder.layers.encoder_layer_10.mlp.3, base_model.encoder.layers.encoder_layer_10.mlp.4, base_model.encoder.layers.encoder_layer_10.self_attention, base_model.encoder.layers.encoder_layer_10.self_attention.out_proj, base_model.encoder.layers.encoder_layer_11, base_model.encoder.layers.encoder_layer_11.dropout, base_model.encoder.layers.encoder_layer_11.ln_1, base_model.encoder.layers.encoder_layer_11.ln_2, base_model.encoder.layers.encoder_layer_11.mlp, base_model.encoder.layers.encoder_layer_11.mlp.0, base_model.encoder.layers.encoder_layer_11.mlp.1, base_model.encoder.layers.encoder_layer_11.mlp.2, base_model.encoder.layers.encoder_layer_11.mlp.3, base_model.encoder.layers.encoder_layer_11.mlp.4, base_model.encoder.layers.encoder_layer_11.self_attention, base_model.encoder.layers.encoder_layer_11.self_attention.out_proj, base_model.encoder.layers.encoder_layer_2.self_attention.out_proj, base_model.encoder.layers.encoder_layer_3.self_attention.out_proj, base_model.encoder.layers.encoder_layer_4.self_attention.out_proj, base_model.encoder.layers.encoder_layer_5.self_attention.out_proj, base_model.encoder.layers.encoder_layer_6.self_attention.out_proj, base_model.encoder.layers.encoder_layer_7.self_attention.out_proj, base_model.encoder.layers.encoder_layer_8.self_attention.out_proj, base_model.encoder.layers.encoder_layer_9, base_model.encoder.layers.encoder_layer_9.dropout, base_model.encoder.layers.encoder_layer_9.ln_1, base_model.encoder.layers.encoder_layer_9.ln_2, base_model.encoder.layers.encoder_layer_9.mlp, base_model.encoder.layers.encoder_layer_9.mlp.0, base_model.encoder.layers.encoder_layer_9.mlp.1, base_model.encoder.layers.encoder_layer_9.mlp.2, base_model.encoder.layers.encoder_layer_9.mlp.3, base_model.encoder.layers.encoder_layer_9.mlp.4, base_model.encoder.layers.encoder_layer_9.self_attention, base_model.encoder.layers.encoder_layer_9.self_attention.out_proj, base_model.encoder.ln, base_model.heads, base_model.heads.head, classifiers.0, classifiers.1, classifiers.2, classifiers.3, classifiers.4, classifiers.5, classifiers.6, classifiers.7, classifiers.9, ees.0, ees.0.0, ees.0.0.dropout, ees.0.0.ln_1, ees.0.0.ln_2, ees.0.0.mlp, ees.0.0.mlp.0, ees.0.0.mlp.1, ees.0.0.mlp.2, ees.0.0.mlp.3, ees.0.0.mlp.4, ees.0.0.self_attention, ees.0.0.self_attention.out_proj, ees.0.1, ees.1, ees.1.0, ees.1.0.dropout, ees.1.0.ln_1, ees.1.0.ln_2, ees.1.0.mlp, ees.1.0.mlp.0, ees.1.0.mlp.1, ees.1.0.mlp.2, ees.1.0.mlp.3, ees.1.0.mlp.4, ees.1.0.self_attention, ees.1.0.self_attention.out_proj, ees.1.1, ees.2, ees.2.0, ees.2.0.dropout, ees.2.0.ln_1, ees.2.0.ln_2, ees.2.0.mlp, ees.2.0.mlp.0, ees.2.0.mlp.1, ees.2.0.mlp.2, ees.2.0.mlp.3, ees.2.0.mlp.4, ees.2.0.self_attention, ees.2.0.self_attention.out_proj, ees.2.1, ees.3, ees.3.0, ees.3.0.dropout, ees.3.0.ln_1, ees.3.0.ln_2, ees.3.0.mlp, ees.3.0.mlp.0, ees.3.0.mlp.1, ees.3.0.mlp.2, ees.3.0.mlp.3, ees.3.0.mlp.4, ees.3.0.self_attention, ees.3.0.self_attention.out_proj, ees.3.1, ees.4, ees.4.0, ees.4.0.dropout, ees.4.0.ln_1, ees.4.0.ln_2, ees.4.0.mlp, ees.4.0.mlp.0, ees.4.0.mlp.1, ees.4.0.mlp.2, ees.4.0.mlp.3, ees.4.0.mlp.4, ees.4.0.self_attention, ees.4.0.self_attention.out_proj, ees.4.1, ees.5, ees.5.0, ees.5.0.dropout, ees.5.0.ln_1, ees.5.0.ln_2, ees.5.0.mlp, ees.5.0.mlp.0, ees.5.0.mlp.1, ees.5.0.mlp.2, ees.5.0.mlp.3, ees.5.0.mlp.4, ees.5.0.self_attention, ees.5.0.self_attention.out_proj, ees.5.1, ees.6, ees.6.0, ees.6.0.dropout, ees.6.0.ln_1, ees.6.0.ln_2, ees.6.0.mlp, ees.6.0.mlp.0, ees.6.0.mlp.1, ees.6.0.mlp.2, ees.6.0.mlp.3, ees.6.0.mlp.4, ees.6.0.self_attention, ees.6.0.self_attention.out_proj, ees.6.1, ees.7, ees.7.0, ees.7.0.dropout, ees.7.0.ln_1, ees.7.0.ln_2, ees.7.0.mlp, ees.7.0.mlp.0, ees.7.0.mlp.1, ees.7.0.mlp.2, ees.7.0.mlp.3, ees.7.0.mlp.4, ees.7.0.self_attention, ees.7.0.self_attention.out_proj, ees.7.1, ees.8.0.self_attention.out_proj, ees.9, ees.9.0, ees.9.0.dropout, ees.9.0.ln_1, ees.9.0.ln_2, ees.9.0.mlp, ees.9.0.mlp.0, ees.9.0.mlp.1, ees.9.0.mlp.2, ees.9.0.mlp.3, ees.9.0.mlp.4, ees.9.0.self_attention, ees.9.0.self_attention.out_proj, ees.9.1\n",
      "Unsupported operator aten::mul encountered 45 time(s)\n",
      "Unsupported operator aten::add encountered 23 time(s)\n",
      "Unsupported operator aten::div encountered 11 time(s)\n",
      "Unsupported operator aten::unflatten encountered 11 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 11 time(s)\n",
      "Unsupported operator aten::gelu encountered 11 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "base_model.encoder.layers.encoder_layer_0.self_attention.out_proj, base_model.encoder.layers.encoder_layer_1.self_attention.out_proj, base_model.encoder.layers.encoder_layer_10, base_model.encoder.layers.encoder_layer_10.dropout, base_model.encoder.layers.encoder_layer_10.ln_1, base_model.encoder.layers.encoder_layer_10.ln_2, base_model.encoder.layers.encoder_layer_10.mlp, base_model.encoder.layers.encoder_layer_10.mlp.0, base_model.encoder.layers.encoder_layer_10.mlp.1, base_model.encoder.layers.encoder_layer_10.mlp.2, base_model.encoder.layers.encoder_layer_10.mlp.3, base_model.encoder.layers.encoder_layer_10.mlp.4, base_model.encoder.layers.encoder_layer_10.self_attention, base_model.encoder.layers.encoder_layer_10.self_attention.out_proj, base_model.encoder.layers.encoder_layer_11, base_model.encoder.layers.encoder_layer_11.dropout, base_model.encoder.layers.encoder_layer_11.ln_1, base_model.encoder.layers.encoder_layer_11.ln_2, base_model.encoder.layers.encoder_layer_11.mlp, base_model.encoder.layers.encoder_layer_11.mlp.0, base_model.encoder.layers.encoder_layer_11.mlp.1, base_model.encoder.layers.encoder_layer_11.mlp.2, base_model.encoder.layers.encoder_layer_11.mlp.3, base_model.encoder.layers.encoder_layer_11.mlp.4, base_model.encoder.layers.encoder_layer_11.self_attention, base_model.encoder.layers.encoder_layer_11.self_attention.out_proj, base_model.encoder.layers.encoder_layer_2.self_attention.out_proj, base_model.encoder.layers.encoder_layer_3.self_attention.out_proj, base_model.encoder.layers.encoder_layer_4.self_attention.out_proj, base_model.encoder.layers.encoder_layer_5.self_attention.out_proj, base_model.encoder.layers.encoder_layer_6.self_attention.out_proj, base_model.encoder.layers.encoder_layer_7.self_attention.out_proj, base_model.encoder.layers.encoder_layer_8.self_attention.out_proj, base_model.encoder.layers.encoder_layer_9.self_attention.out_proj, base_model.encoder.ln, base_model.heads, base_model.heads.head, classifiers.0, classifiers.1, classifiers.2, classifiers.3, classifiers.4, classifiers.5, classifiers.6, classifiers.7, classifiers.8, ees.0, ees.0.0, ees.0.0.dropout, ees.0.0.ln_1, ees.0.0.ln_2, ees.0.0.mlp, ees.0.0.mlp.0, ees.0.0.mlp.1, ees.0.0.mlp.2, ees.0.0.mlp.3, ees.0.0.mlp.4, ees.0.0.self_attention, ees.0.0.self_attention.out_proj, ees.0.1, ees.1, ees.1.0, ees.1.0.dropout, ees.1.0.ln_1, ees.1.0.ln_2, ees.1.0.mlp, ees.1.0.mlp.0, ees.1.0.mlp.1, ees.1.0.mlp.2, ees.1.0.mlp.3, ees.1.0.mlp.4, ees.1.0.self_attention, ees.1.0.self_attention.out_proj, ees.1.1, ees.2, ees.2.0, ees.2.0.dropout, ees.2.0.ln_1, ees.2.0.ln_2, ees.2.0.mlp, ees.2.0.mlp.0, ees.2.0.mlp.1, ees.2.0.mlp.2, ees.2.0.mlp.3, ees.2.0.mlp.4, ees.2.0.self_attention, ees.2.0.self_attention.out_proj, ees.2.1, ees.3, ees.3.0, ees.3.0.dropout, ees.3.0.ln_1, ees.3.0.ln_2, ees.3.0.mlp, ees.3.0.mlp.0, ees.3.0.mlp.1, ees.3.0.mlp.2, ees.3.0.mlp.3, ees.3.0.mlp.4, ees.3.0.self_attention, ees.3.0.self_attention.out_proj, ees.3.1, ees.4, ees.4.0, ees.4.0.dropout, ees.4.0.ln_1, ees.4.0.ln_2, ees.4.0.mlp, ees.4.0.mlp.0, ees.4.0.mlp.1, ees.4.0.mlp.2, ees.4.0.mlp.3, ees.4.0.mlp.4, ees.4.0.self_attention, ees.4.0.self_attention.out_proj, ees.4.1, ees.5, ees.5.0, ees.5.0.dropout, ees.5.0.ln_1, ees.5.0.ln_2, ees.5.0.mlp, ees.5.0.mlp.0, ees.5.0.mlp.1, ees.5.0.mlp.2, ees.5.0.mlp.3, ees.5.0.mlp.4, ees.5.0.self_attention, ees.5.0.self_attention.out_proj, ees.5.1, ees.6, ees.6.0, ees.6.0.dropout, ees.6.0.ln_1, ees.6.0.ln_2, ees.6.0.mlp, ees.6.0.mlp.0, ees.6.0.mlp.1, ees.6.0.mlp.2, ees.6.0.mlp.3, ees.6.0.mlp.4, ees.6.0.self_attention, ees.6.0.self_attention.out_proj, ees.6.1, ees.7, ees.7.0, ees.7.0.dropout, ees.7.0.ln_1, ees.7.0.ln_2, ees.7.0.mlp, ees.7.0.mlp.0, ees.7.0.mlp.1, ees.7.0.mlp.2, ees.7.0.mlp.3, ees.7.0.mlp.4, ees.7.0.self_attention, ees.7.0.self_attention.out_proj, ees.7.1, ees.8, ees.8.0, ees.8.0.dropout, ees.8.0.ln_1, ees.8.0.ln_2, ees.8.0.mlp, ees.8.0.mlp.0, ees.8.0.mlp.1, ees.8.0.mlp.2, ees.8.0.mlp.3, ees.8.0.mlp.4, ees.8.0.self_attention, ees.8.0.self_attention.out_proj, ees.8.1, ees.9.0.self_attention.out_proj\n",
      "Unsupported operator aten::mul encountered 49 time(s)\n",
      "Unsupported operator aten::add encountered 25 time(s)\n",
      "Unsupported operator aten::div encountered 12 time(s)\n",
      "Unsupported operator aten::unflatten encountered 12 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "base_model.encoder.layers.encoder_layer_0.self_attention.out_proj, base_model.encoder.layers.encoder_layer_1.self_attention.out_proj, base_model.encoder.layers.encoder_layer_10.self_attention.out_proj, base_model.encoder.layers.encoder_layer_11.self_attention.out_proj, base_model.encoder.layers.encoder_layer_2.self_attention.out_proj, base_model.encoder.layers.encoder_layer_3.self_attention.out_proj, base_model.encoder.layers.encoder_layer_4.self_attention.out_proj, base_model.encoder.layers.encoder_layer_5.self_attention.out_proj, base_model.encoder.layers.encoder_layer_6.self_attention.out_proj, base_model.encoder.layers.encoder_layer_7.self_attention.out_proj, base_model.encoder.layers.encoder_layer_8.self_attention.out_proj, base_model.encoder.layers.encoder_layer_9.self_attention.out_proj, classifiers.0, classifiers.1, classifiers.2, classifiers.3, classifiers.4, classifiers.5, classifiers.6, classifiers.7, classifiers.8, classifiers.9, ees.0, ees.0.0, ees.0.0.dropout, ees.0.0.ln_1, ees.0.0.ln_2, ees.0.0.mlp, ees.0.0.mlp.0, ees.0.0.mlp.1, ees.0.0.mlp.2, ees.0.0.mlp.3, ees.0.0.mlp.4, ees.0.0.self_attention, ees.0.0.self_attention.out_proj, ees.0.1, ees.1, ees.1.0, ees.1.0.dropout, ees.1.0.ln_1, ees.1.0.ln_2, ees.1.0.mlp, ees.1.0.mlp.0, ees.1.0.mlp.1, ees.1.0.mlp.2, ees.1.0.mlp.3, ees.1.0.mlp.4, ees.1.0.self_attention, ees.1.0.self_attention.out_proj, ees.1.1, ees.2, ees.2.0, ees.2.0.dropout, ees.2.0.ln_1, ees.2.0.ln_2, ees.2.0.mlp, ees.2.0.mlp.0, ees.2.0.mlp.1, ees.2.0.mlp.2, ees.2.0.mlp.3, ees.2.0.mlp.4, ees.2.0.self_attention, ees.2.0.self_attention.out_proj, ees.2.1, ees.3, ees.3.0, ees.3.0.dropout, ees.3.0.ln_1, ees.3.0.ln_2, ees.3.0.mlp, ees.3.0.mlp.0, ees.3.0.mlp.1, ees.3.0.mlp.2, ees.3.0.mlp.3, ees.3.0.mlp.4, ees.3.0.self_attention, ees.3.0.self_attention.out_proj, ees.3.1, ees.4, ees.4.0, ees.4.0.dropout, ees.4.0.ln_1, ees.4.0.ln_2, ees.4.0.mlp, ees.4.0.mlp.0, ees.4.0.mlp.1, ees.4.0.mlp.2, ees.4.0.mlp.3, ees.4.0.mlp.4, ees.4.0.self_attention, ees.4.0.self_attention.out_proj, ees.4.1, ees.5, ees.5.0, ees.5.0.dropout, ees.5.0.ln_1, ees.5.0.ln_2, ees.5.0.mlp, ees.5.0.mlp.0, ees.5.0.mlp.1, ees.5.0.mlp.2, ees.5.0.mlp.3, ees.5.0.mlp.4, ees.5.0.self_attention, ees.5.0.self_attention.out_proj, ees.5.1, ees.6, ees.6.0, ees.6.0.dropout, ees.6.0.ln_1, ees.6.0.ln_2, ees.6.0.mlp, ees.6.0.mlp.0, ees.6.0.mlp.1, ees.6.0.mlp.2, ees.6.0.mlp.3, ees.6.0.mlp.4, ees.6.0.self_attention, ees.6.0.self_attention.out_proj, ees.6.1, ees.7, ees.7.0, ees.7.0.dropout, ees.7.0.ln_1, ees.7.0.ln_2, ees.7.0.mlp, ees.7.0.mlp.0, ees.7.0.mlp.1, ees.7.0.mlp.2, ees.7.0.mlp.3, ees.7.0.mlp.4, ees.7.0.self_attention, ees.7.0.self_attention.out_proj, ees.7.1, ees.8, ees.8.0, ees.8.0.dropout, ees.8.0.ln_1, ees.8.0.ln_2, ees.8.0.mlp, ees.8.0.mlp.0, ees.8.0.mlp.1, ees.8.0.mlp.2, ees.8.0.mlp.3, ees.8.0.mlp.4, ees.8.0.self_attention, ees.8.0.self_attention.out_proj, ees.8.1, ees.9, ees.9.0, ees.9.0.dropout, ees.9.0.ln_1, ees.9.0.ln_2, ees.9.0.mlp, ees.9.0.mlp.0, ees.9.0.mlp.1, ees.9.0.mlp.2, ees.9.0.mlp.3, ees.9.0.mlp.4, ees.9.0.self_attention, ees.9.0.self_attention.out_proj, ees.9.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2908083456,\n",
       " 4303940352,\n",
       " 5699797248,\n",
       " 7095654144,\n",
       " 8491511040,\n",
       " 9887367936,\n",
       " 11283224832,\n",
       " 12679081728,\n",
       " 14074938624,\n",
       " 15470795520,\n",
       " 16866652416]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from torchinfo import summary    #ViT는 summary FLOPs 계산이 100분의 1배로 계산나옴...\n",
    "tmp=[]\n",
    "for i in range(exit_num):\n",
    "    model.set_each_ee_test_mode(i)\n",
    "    #summary(model,input_size= (1, 3, IMG_SIZE, IMG_SIZE))\n",
    "    tmp.append(summary(model,input_size= (1, 3, IMG_SIZE, IMG_SIZE)).total_mult_adds)\n",
    "tmp''' \n",
    "\n",
    "tmp = []\n",
    "for i in range(exit_num):\n",
    "    model.set_each_ee_test_mode(i)\n",
    "    x = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(device)\n",
    "    flops = FlopCountAnalysis(model, x)\n",
    "    tmp.append(flops.total())\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7799667684"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ResNet101 FLOPs: 16866721536\n",
    "# 7799667684(resnet101) VS 16866721536(ViT-b/16)\n",
    "from torchinfo import summary\n",
    "pretrained_resnet101 = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "pretrained_resnet101.fc = nn.Linear(pretrained_resnet101.fc.in_features, dataset_outdim[data_choice])\n",
    "summary(pretrained_resnet101,input_size= (1, 3, IMG_SIZE, IMG_SIZE)).total_mult_adds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216.24923290770295 %\n"
     ]
    }
   ],
   "source": [
    "print(16866721536 / 7799667684 * 100,'%') #ViT-b/16의 flops가 resnet101의 216.24%이다.. 즉 2배이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182.80294075046498 %\n"
     ]
    }
   ],
   "source": [
    "# 만약 앙상블을 한다면 총 연산량은 단일출구에 비해 1.8배가 될 것이다.\n",
    "print(30832862976 / 16866721536 * 100,'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
